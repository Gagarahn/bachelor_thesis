\documentclass[a4paper,12pt]{article}

\usepackage{a4wide}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}  % For including images
\usepackage{subfigure}  % In preamble
\usepackage{bm}
\usepackage{xcolor}  % For a colorfull presentation
\usepackage{listings}  % For presenting code 


\usepackage[english, science, dropcaps, hyperref, submissionstatement]{ku-frontpage}
\usepackage[utf8]{inputenc}
\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{0}


% Definition of a style for code, matter of taste
\lstdefinestyle{mystyle}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color[HTML]{F7F7F7},
  rulecolor=\color[HTML]{EEEEEE},
  identifierstyle=\color[HTML]{24292E},
  emphstyle=\color[HTML]{005CC5},
  keywordstyle=\color[HTML]{D73A49},
  commentstyle=\color[HTML]{6A737D},
  stringstyle=\color[HTML]{032F62},
  emph={@property,self,range,True,False},
  morekeywords={super,with,as,lambda, for, do, end, return, if, then},
  literate=%
    {+}{{{\color[HTML]{D73A49}+}}}1
    {-}{{{\color[HTML]{D73A49}-}}}1
    {*}{{{\color[HTML]{D73A49}*}}}1
    {/}{{{\color[HTML]{D73A49}/}}}1
    {=}{{{\color[HTML]{D73A49}=}}}1
    {/=}{{{\color[HTML]{D73A49}=}}}1,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=none,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=4,
  frame=single,
}
\lstset{style=mystyle}

\assignment{Bachelor Thesis}
\author{Hans-Christian Zerrahn}


\title{Spatiotemporal Prediction of Mycelial\\Growth Using ConvLSTM Networks}
\subtitle{A Study on Long-Term Forecasting of Skeletonized Yeast Networks}
\date{Submitted: \today}
\advisor{Advisor: Helle SÃ¸rensen}
\frontpageimage{example.jpg}

\kupdfsetup{Spatiotemporal Prediction of Mycelial Growth Using ConvLSTM Networks - A Study on Long-Term Forecasting of Skeletonized Yeast Networks}{}{Hans-Christian Zerrahn}

\begin{document}
\begingroup
  \fontencoding{T1}\fontfamily{LinuxLibertineT-OsF}\selectfont
  \maketitle
\endgroup

\section{Abstract}
This thesis explores ConvLSTM's ability to predict the growth pattern of the mycelium network associated with the yeast \textit{Saccharomyces cerevisiae} over time. The sequences of growth have been binarized and then skeletonized so only the mycelium network remains; this is the data the models are trained on and the form of the predictions as well.
It's an autoregressive model making up to $80$-frames long predictions, the models vary in depth, width, prediction horizon, and the foreground percentage of the datasets, they are trained on. The reasoning behind the variation is to test the models' performance under different conditions.
We found that deeper and wider models trained on a sparse foreground dataset had the best performance when evaluating loss statistics and qualitative visual data. The best $80$-frame prediction model had $64$ hidden channels and $3$ layers, it had an average dice score of $0.7805$, and visually seems to capture the spatial features very good and the temporal features decently.
Generally, the models all seemed to suffer from probabilistic blur in the later part of the prediction sequence. This made the models unable to learn and predict the intrinsic thin hyphae growth in the mycelium network. The findings seem to suggest that mycelium networks' growth over time might be more complex than expected, and that more complex models achieve better results.
Overall, the modelling of complex mycelium networks seems feasible using ConvLSTM. Directions for future work could include deeper and wider models as well as the implementation of transformer architecture.


\section{Acknowledgements}
Part of the computation done for this project was performed on the UCloud interactive HPC system, which is managed by the eScience Center at the University of Southern Denmark.
\newpage
% Please leave the table of contents as is, for the ease of navigation for TAs
\tableofcontents % Generates the table of contents
\newpage % Start a new page after the table of contents

\section{Introduction}
Deep learning neural networks have proven to be an effective method for spatiotemporal forecasting. Being able to make accurate predictions of future events has proven valuable; it prevents mistakes and improves decision quality. In this thesis, we will be applying the methods of deep learning to make predictions on mycelium growth patterns.
To be more precise, based on sequential images of the initial growth of mycelium, the model will output a prediction sequence of images of future growth. Being able to predict the future growth of mycelium has many use cases. Firstly, one would be able to predict whether or not the mycelium will keep growing, which helps determine if the sample should keep growing or be scratched. Secondly, mycelium networks have been shown to grow optimally for energy conservation in certain settings, those networks are also optimal for pathways in subway systems \cite{joseph2022slime}.
This further adds incentives to learn and study the growth of mycelium networks. For this objective, we will be using Convolutional Long Short-Term Memory (ConvLSTM) networks, because they combine the capturing of temporal (LSTM) and spatial (Convolution) dependencies, which is something most time-series models are not capable of. There is a general problem with Deep Learning models getting stuck in local minima during training. Moreover, when making long prediction sequences, there is usually a lot of variance, and predictions get imprecise. At last, the number one problem with spatiotemporal models is their heavy memory usage.
The topic of this thesis is to study how effective the ConvLSTM is at making predictions of variable sequence length. What effect do the different hyperparameters, have on the models' soundness?

\section{Theoretical background}


\subsection{Loss function}
The loss function is the function used to measure error. There are plenty of loss functions to pick from, depending on the scenario; some are more appropriate than others.
Since the loss function is what we are measuring our error on and are using to optimize our model, the choice of loss function matters a lot. The notation used for loss functions throughout this paper is $L(f(x; \theta), y)$ where $f$ is the model making a prediction. The general goal is for $f(x; \theta) \approx y$, which is what we hope to achieve by minimizing the loss function.
One has to be aware of overfitting and keep in mind that most outcomes have a variance; therefore, a bit of error is acceptable, and the goal in reality is to try and find a model that captures the underlying/true structure of the data.
\subsubsection{Dice loss}
The Dice Score is a measure of how many elements two sets have in common
\begin{align}
\text{Dice Score} =\frac{2|P \cap T|}{|P|+|T|}
\end{align}




The principle of the dice Score can be transformed into a loss function called the dice loss. It looks at the predicted matrix and the target matrix and compares the values of the same entry.


The Dice score and the fraction representing the Dice Score below are equivalent for binary inputs.
\begin{align}
L_{\text {dice }}=1-\frac{2 \sum_{m,n} t_{m,n} p_{m,n}}{\sum_{m,n}\left(t_{m,n}+p_{m,n}\right)}
\end{align}
The Dice score is equal to one when we have the same values and zero when one of the values is zero.
\subsubsection{Binary focal weight}
The binary focal weight is used to increase the loss for hard predictions and reduce the loss for easy predictions. Firstly, the probability of the predictions is found
\begin{align}
\hat{P} = \sigma(\hat{y})
\end{align}
We then use this to find the probability of the true class
\begin{align}
P_t = y \hat{P} + (1-y)(1-\hat{P})
\end{align}
The binary focal weight is then
\begin{align}
\text{Focal weight} = (1-P_t)^\gamma
\end{align}
$P_t$ is close to one when the predicted probability of the true class is close to one. This makes it so this $(1-P_t)^\gamma$ is close to zero, which is what we want because if $P_t \approx 1$ then it must be an easy prediction. However, if $P_t$ is close to zero, then it must be a hard prediction to make, so the focal weight is closer to one, which isn't punished as hard. Typically $\gamma > 1$ because we want to punish lower values of $P_t$ harder in the loss.








% The function of distance weight is implied in the name. It is used to punish predictions that are further away from the target on an image harder than those close to the target. Imagine a binary matrix where zeroes are background and ones are foreground, then a matrix is created with the Euclidean distance calculated to the nearest foreground pixel for each entrance in the input matrix.




% We create a binary mask $M \in {0,1}^{H \times W}$, where $M_{i,j} = 1$ is foreground and $M_{i,j} = 0$ is background. We get the following
% \begin{align}
%   D(i, j) = \min_{i^\prime, j^\prime} \sqrt{(i-i^\prime)^2+(j-j^\prime)^2} \text{   Given that    } M_{i^\prime, j^\prime} = 1
% \end{align}
% This is the distance weight for entrance $i, j$




\subsubsection{Binary Cross Entropy with sigmoid layer}
The BCE loss is for binary ground truth labels and prediction labels $p \in [0,1]$.
\begin{align}
\text{BCE}(\hat{y}, y) = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
\end{align}
Intuitively, we can see that the model punishes wrong guesses hard due to how the natural logarithm scale for values in the range $[0;1]$.




Taking the sigmoid function of the prediction works extremely well because it maps the prediction to values in $[0,1]$. This is advantageous because it is a binary event, and we want to know the probability of a pixel being one or zero. The fact that $\sigma(0)=0.5$ makes it great for binary tasks, because we can then use zero as a threshold for classification. Further, it is differentiable and smooth, which is important since we have to find the gradient of the loss function.
\begin{align}
l_{n}&=\left[p y_{n} \cdot \log (\sigma\left(x_{n}\right))+\left(1-y_{n}\right) \cdot \log \left(1-\sigma\left(x_{n}\right)\right)\right]
\end{align}
We are further adding a weight to the loss, this is done to adjust for class imbalance. If there is a majority of one class, it can be a problem because it's then safer to bet on it being that class, so the weight is making up for the class imbalance.
\begin{align}
p &= \frac{N-\sum^{N}_{n=1} y_n }{\sum^{N}_{n=1} y_n}
\end{align}
\begin{align}
\ell(x, y)&=\frac{1}{N} \sum^{N}_{n=1} l_{n}
\end{align}
In the end, we average over the loss for each pixel and get the average error on the prediction.




This loss is good for prioritizing generating correct pixels, binary prediction tasks, and reducing the problem of class imbalance.




\subsection{Backpropagation}
The backbone of the Backpropagation algorithm is the chain rule.




Chain rule: If $g$ is differentiable at $x$ and $f$ is differentiable at $g(x)$, then the composite function $h=f \circ g$ is differentiable at $x$ and $h^\prime(x)$ is given by:
\begin{align}
h^\prime(x)=f^\prime(g(x))\cdot g^\prime(x)
\end{align}
Using Leibniz notation, which is the notation moving forward, we have:
\begin{align}
\frac{d f}{d x} = \frac{d f}{d g } \frac{d g}{d x}
\end{align}
We define $u^{(i)}$ as
\begin{align}
u^{(i)} = f^{(i)}(\mathbb{A}^{(i)})
\end{align}
Where $\mathbb{A}^{(i)}$ is the set of all nodes that are parents to $u^{(i)}$. This is the notation from \cite{Goodfellow-et-al-2016}, which we are basing this section on.




Since we are looking at graphs, only the parents of the node serve as input to the function associated with that node.
We consider a graph describing how to compute a scalar $u^{(n)}$ at node n, this scalar is the one we want to find the gradient of with respect to its $n_i+n$ ancestors $u^{(1)}$ to $u^{(n_i)}$, ie. we want to find $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ for it's $n_i$ ancestors: $i \in \{1,2,\dots, n_i\}$.
With the assumption that the nodes in the graph have been ordered in a way that we can compute the output $u^{(t)}$ for $t \in \{n_i+1, n_i+2,\dots, n\}$ one after the other.




We can, per the chain rule, compute partial derivatives as
\begin{align}
\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i : j \in \text{Pa}(u^{(i)})} \frac{\partial u^{(n)}}{\partial u^{(i)}} \frac{\partial u^{(i)}}{\partial u^{(j)}}
\end{align}




Using the above, we can derive the pseudo-code for the back propagation algorithm
\begin{algorithm}[H]
\caption{Simplified Backpropagation Algorithm }\label{alg:simple-backprop}
\begin{algorithmic}[1]
\State use   $u^{(i)} = f^{(i)}(\mathbb{A}^{(i)})$ to obtain the activations of the network.
\State Initialize \texttt{grad\_table}, a data structure that will store the derivatives that have been computed.
\Statex \hspace{1.5em} The entry \texttt{grad\_table}[$u^{(i)}$] stores \( \frac{\partial u^{(n)}}{\partial u^{(i)}} \)
\State \texttt{grad\_table}[$u^{(n)}$] $\gets 1$
\For{$j = n-1$ \textbf{downto} $1$}
  \Statex \hspace{1.5em} Compute:
  \[
    \frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i : j \in \mathrm{Pa}(u^{(i)})} \frac{\partial u^{(n)}}{\partial u^{(i)}} \cdot \frac{\partial u^{(i)}}{\partial u^{(j)}}
  \]
  \State \texttt{grad\_table}[$u^{(j)}$] $\gets \sum_{i : j \in \mathrm{Pa}(u^{(i)})}$ \texttt{grad\_table}[$u^{(i)}$] $\cdot \frac{\partial u^{(i)}}{\partial u^{(j)}}$
\EndFor
\State \Return $\{\texttt{grad\_table}[u^{(i)}] \mid i = 1, \dots, n_i\}$
\end{algorithmic}
\end{algorithm}




\subsection{Gradient descent}
Gradient descent is an optimization algorithm that converges towards a minimum by moving in the direction of the negative gradient. This section follows \cite{ruder2016overview} explanation of gradient descent and its variants.
The three primary types of gradient descent are:
\begin{enumerate}
\item \textbf{Batch gradient descent:}
This is the standard gradient descent method. It computes the gradient of the loss function with respect to the parameters of the loss function for the entire dataset.
Since it has to calculate gradients on the entire dataset, it can be very slow, and in a lot of cases, the dataset will be too big to fit into memory, which will cause the program to crash.
\begin{align}
\theta = \theta - \eta \cdot \nabla_\theta L(\theta)
\end{align}
$\theta$ represents the parameters/weights, $\eta$ is the learning rate, and $\nabla_\theta L(\theta)$ is the gradient of the loss function with respect to the parameters.
This method will always converge to a local minimum.




\item \textbf{Stochastic gradient descent (SGD):}
This is a commonly used method for updating parameters, it updates the parameters for each sample in the dataset
\begin{align}
\theta = \theta - \eta \cdot \nabla_\theta L(\theta; x^{(i)}; y^{(i)})
\end{align}
This saves a lot of computations and is therefore faster, because it doesn't have to do them on the entire dataset. It does, however, have the effect that the more frequent updates have a higher variance, which causes the weights to fluctuate.
These fluctuations can cause the parameters to jump to a different local minimum and escape the crevasse it was in. The high variance can make it harder for the parameters to converge to a minimum, but this can be combated by decreasing the learning rate over time. It has to be mentioned that for each epoch, we shuffle the dataset, so as to avoid the model learning the order of the dataset and to keep it unbiased.




\item \textbf{Mini-batch gradient descent:}
Minibatch gradient descent is a combination of both of the above. It takes more than just a single sample and less than the entire dataset. This deals with the problems in both methods, it reduces the computation time, helps with memory management, and reduces variance, which helps with convergence.
\begin{align}
  \theta = \theta - \eta \cdot \frac{1}{n-i}\nabla_\theta \sum_{j = i}^{n} L(\theta; x^{(j)}; y^{(j)})
\end{align}
The batch size can vary greatly; the smaller the size, the fewer computations and less memory used, but this also results in higher variance.
When using mini-batches, different deep learning libraries leverage super-optimized linear algebra methods combined with parallel computations of samples using GPUs.
\end{enumerate}
\subsubsection{ADAM}
The Adaptive Moment Estimation (ADAM) method is used for computing adaptive learning rates of the individual parameters.
\begin{align}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \frac{1}{n-i}\nabla_\theta \sum_{j = i}^{n} L(\theta; x^{(j)}; y^{(j)}) \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) (\frac{1}{n-i}\nabla_\theta \sum_{j = i}^{n} L(\theta; x^{(j)}; y^{(j)}))^2
\end{align}
$m_t$ and $v_t$ are estimates of the mean and the variance of the gradients, respectively. As we can see from the formula, the estimate of the moments is dependent upon the estimate of the moments of previous timesteps. $m_t$ and $v_t$ are initialized as vectors of zero, thus making them biased towards zero; the bias is extra present during initial time steps and when $\beta_1$ and $\beta_2$ are close to one.
ADAM combats these biases by using bias-corrected mean and variance estimates.
\begin{align}
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t}
\end{align}
This gives the following ADAM update rule:
\begin{align}
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}




\subsection{Neural Networks}
A neural network is a machine learning model type that mimics the structure and function of biological neural networks. Much like the brain of a living being, it is composed of neurons and synapses. The neurons communicate through the signal in synapses, neurons take these signals as inputs, and pass them through an activation function for output.
The neural network has an input layer, one or more hidden layers, and an output layer, this is shown in Figure~\ref{fig:n2}. Each layer is built of neurons stacked on top of each other and has the same way of handling input.


Every layer except the input layer has an activation function. An activation function is typically a non-linear function, this is because if a neural network only has linear activation functions, the entire network is equivalent to a single-layer model.
Typical examples of activation functions include: ReLU, Sigmoid / logistic, hyperbolic tangent, exponential LU. The edges are usually of some weight, which is used to calculate the signal represented by a linear combination of the input.




Multi-layered networks are one of the simplest forms of neural networks and often serve as the standard example. The following example is based on \cite{bishop2024deep} section 6.2 on Multilayer Networks.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{images/NN.png}
\caption{Representation of a multi-layered Neural Network.(Adapted from \cite{bishop2024deep} (p. 181)) } % Dummy caption generated using lipsum
\label{fig:n2}
\end{figure}
The weights of the edges into the neuron define the input into the associated activation function. The superscript $(1)$ is the symbol for being on the first layer and $j=1,\dots, M$.
\begin{align}
a^{(1)}_j = \sum_{i=1}^{D} w^{(1)}_{ji} x_i + w^{(1)}_{j0}
\end{align}
The activation then takes that input and outputs the value of that node.
\begin{align}
z_j^{(1)} = h\left(a_j^{(1)}\right)
\end{align}
Now the value of that node is used for the calculation of the input to the next activation function.
\begin{align}
a_k^{(2)} = \sum_{j=1}^{M} w_{kj}^{(2)} z_j^{(1)} + w_{k0}^{(2)}
\end{align}
For $l=2,\dots,L-1$ and L being the total number of layers, we can start the recursion of $a_j^{(l+1)}$ and $z_j^{(l)} $
\begin{align}
z_j^{(l)} = h_l\left(a_j^{(l)}\right) \\
a_j^{(l+1)} = \sum_i^M w_{ji}^{(l+1)} z_i^{(l)} = \sum_i^M w_{ji}^{(l+1)} h_l\left(a_i^{(l)}\right)
\end{align}
Then it is easy to find the output
\begin{align}
y_j = h_L\left(a_j^{(L)}\right)
\end{align}
It is crucial to keep in mind that neural network (NN) is an umbrella term, and therefore, the term can be somewhat loose, and two neural networks can be vastly different.


\subsection{Recurrent neural networks}
Recurrent neural networks (RNNs) are a subcategory of neural networks, designed for sequential data, so that previous hidden states used to find outputs can be used as inputs for the next timestep.
This makes it so the model can retain important information from the previous time step and use that information in the calculations of the output of the current time step.
The following is based upon \cite{amidi_rnn_cheatsheet}, which provides an introduction to RNNs.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{images/architecture-rnn-ltr.png}
\caption{Representation of the architecture for a typical recurrent neural network (RNN), (Adapted from \cite{amidi_rnn_cheatsheet})} % Dummy caption generated using lipsum
\label{fig:n3}
\end{figure}
Observing Figure~\ref{fig:n3} should remind you of a Hidden Markov Model (HMM) because of the sequential structure and temporal dependence; one can think of RNNs as the differentiable version of an HMM.




Typically, the activation / hidden state $a^{\langle t \rangle}$ will look like a variation of this, for some activation function $\sigma_1$:
\begin{align}
a^{\langle t \rangle} = \sigma_1\left(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a\right)
\end{align}
The hidden state is used as part of the input to the activation function $\sigma_2$ for the calculation of the output $y^{\langle t \rangle}$
\begin{align}
y^{\langle t \rangle} = \sigma_2\left(W_{ya} a^{\langle t \rangle} + b_y\right)
\end{align}
Where the weights $W_{aa}, W_{ax}, W_{ya}$ and biases $b_a, b_y$ are shared temporally.




What typically changes in different RNNs is the way the recurrent update happens. Changes usually occur because you want to make it specific for your task or to combat some problems you're having. One of the most common problems is the Vanishing/exploding gradient phenomenon.
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.8\linewidth]{images/description-block-rnn-ltr.png}
%   \caption{Representation of a vanilla RNN recurrent update block} % Dummy caption generated using lipsum
%   \label{fig:n4}
% \end{figure}
\subsubsection{Exploding/vanishing gradient problem}
In a standard RNN, there are problems with computing the gradient of the loss function with respect to the weights because, as shown in the backpropagation section, we find the gradient as follows:
\begin{align}
\frac{\partial L}{\partial h_k} = \frac{\partial L}{\partial h_t} \cdot \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}
\end{align}
This is a problem for very high values of t because if we look at this term:
\begin{align}
\frac{\partial h_i}{\partial h_{i-1}}
\end{align}
In this matrix, for the eigenvalues that are less than one, they will vanish, and the ones that are greater than one will explode. This is because the term is multiplied together in a long, consecutive chain.
We observe that for $\frac{\partial h_i}{\partial h_{i-1}}$ with all eigenvalues less than one:
\begin{align}
\left\| \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} \right\| \to 0 \quad \text{as } t - k \to \infty
\end{align}
Conversely, for eigenvalues greater than one, we observe:
\begin{align}
\left\| \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} \right\| \to \infty \quad \text{as } t - k \to \infty
\end{align}
This is known as the exploding/vanishing gradient problem typical for RNNs.
\subsection{Long short-term memory}
The long short-term memory (LSTM) network, which is a gated RNN, was made to fix the problem RNNs had with exploding/vanishing gradients.
Instead of recurrently connecting hidden units like in a normal RNN, LSTM connects cell blocks made up of non-linear and linear operations on different gates.
An LSTM cell consists of an input, three gates, a cell update, a cell state, and a hidden state (the output). The theory in this section is based on \cite{Goodfellow-et-al-2016} chapter 10.9 and 10.10 on Long Short-Term Memory (LSTM).




A gate is a learned mechanism for controlling the amount of information that flows through a cell.
There are three different types of gates in a cell in an LSTM: the forget gate decides how much of old information to keep, the input gate decides how much of new information to introduce into memory, and the output gate decides how much of the memory we want to output.
\begin{align}
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\end{align}
Where $f_t$ is the forget gate, $i_t$ is the input gate, $o_t$ is the output gate, and $\sigma$ is the sigmoid function.




The cell update is the mechanism deciding how we should update our cell state. It has much like the gates, the previous hidden state, and the current input as input variables. It decides what to write in the new cell state based on the new information from the previous cell.
\begin{align}
\tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
\end{align}




The cell state can be intuitively seen as the long-term memory of the network. Each cell in the recurrent network has a state that symbolizes the memory at that time. The information flows chronologically up through the cells, and at each cell, it calculates how much information from the previous cell state should be kept and how much new information to put into memory.
The cell state is defined by the forget gate, the previous cell state, the input gate, and the cell update. The forget gate determines how much of the previous cell state to keep, this is usually pretty close to one. The input gate determines how much of the cell update should be added to the cell state.
Due to the current cell states' dependence upon the previous, it makes it so that all cell states at time  $t_n$ are dependent upon all previous cell states at time $t_i<t_n$, unless $f_t=0$, then all memory is wiped and the new chain of dependence starts at $t$.
\begin{align}
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
\end{align}
Where the symbol $\odot$ represents the element-wise product/Hadamard product.




The last part of the cell calculates the hidden state. It is the element-wise product of the output gate and $\tanh(c_t)$. The output gate controls how much of the cell state we want to output. The hidden state is the output of the cell.


\begin{align}
h_t = o_t \odot \tanh(c_t)
\end{align}


This cell architecture is represented in Figure~\ref{fig:n5}, where the inputs, gates, and states are shown with associated operations.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{images/LSTM.png}
\caption{Cell architecture in an LSTM. (Adapted from \cite{Goodfellow-et-al-2016} (p. 405))} % Dummy caption generated using lipsum
\label{fig:n5}
\end{figure}
LSTM solves the vanishing/exploding gradient problem by using a self-loop on the cell states. Because it uses the previous cell state to calculate the current cell state, it creates a linear path for the gradient through time. This is easy to see when we remember how the gradients are calculated through back-propagation or when we explore the vanishing/exploding gradient problem for a general RNN.
\begin{align}
\frac{\partial L}{\partial c_t} = \frac{\partial L}{\partial c_T} \prod_{i=t+1}^{T} \frac{\partial c_t}{\partial c_{t-1}}
\end{align}
But this is no problem because
\begin{align}
\frac{\partial c_t}{\partial c_{t-1}} = f_t
\end{align}
Previously, whenever the gradient hit zero, it would be stuck there, because this is a minimum and therefore optimized, even though it wasn't optimized. Now, what is great about this is that the gradient is controlled by $f_t$, so if one of them is zero, that means things are forgotten, and we are at a minimum for the loss function.
However, because all of the gates are learnable, if the model now learns that it has to retain memory, then it can change the forget gates, and the gradient will no longer be stuck at zero. This is why we usually see $f_t \approx 1$ because the model learns that it is good to retain memory of previous states.
Of course, the exploding gradient is no longer a problem since $0\leq f_t \leq 1$. Since all the weights and biases of the gates and cell updates don't flow through time, the exponential growth or descent doesn't happen in the same way.


\subsection{Convolutional neural networks}
Convolutional neural networks (CNNs) have had huge practical success. A CNN is a neural network that uses convolution instead of matrix multiplication in minimum one of its layers. The material presented in this chapter is adapted from \cite{Goodfellow-et-al-2016} chapter $9$.
\subsubsection{The convolution operator}
The convolution operator is defined as:
\begin{align}
s(t)=(x * w)(t)=\int x(a) w(t-a) d a
\end{align}
Where $x$ is typically our input/data we want changed, and $w$ is a weighting function used to change the input/data.




We have the discrete convolution:
\begin{align}
s(t)=(x * w)(t)=\sum_{a=-\infty}^{\infty} x(a) w(t-a)
\end{align}
This is for when $x$ and $w$ only have integers in their domain.




If we apply the discrete convolution to an image, the image is a two-dimensional matrix, and we then use a two-dimensional kernel as a weight, we get the following:
\begin{align}
S(i, j)=(I * K)(i, j)=\sum \sum I(m, n) K(i-m, j-n)
\end{align}
Since convolution is commutative, due to the flipping of the kernel relative to the input, we can rewrite it as:
\begin{align}
S(i, j)=(K * I)(i, j)=\sum \sum I(i-m, j-n) K(m, n)
\end{align}
This is called 2-D convolution.




However, practically, convolution is rarely used. It is mostly used because of its utility in proofs due to its commutative property.
Most neural network libraries implement the cross-correlation, which doesn't have the commutative property, but is much easier to work with in practice.
\begin{align}
S(i, j)=(K * I)(i, j)=\sum \sum I(i+m, j+n) K(m, n)
\end{align}
Cross-correlation will often be referred to as convolution, because it practically does the same thing.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{images/2dconv.png}
\caption{A general example for 2-D cross-correlation. (Adapted from \cite{Goodfellow-et-al-2016} (p. 330))} % Dummy caption generated using lipsum
\label{fig:n6}
\end{figure}
\subsubsection{Benefits of convolution}
There are three main benefits to using convolution with a kernel.
\begin{enumerate}
\item \textbf{Sparse interactions:} A typical fully connected neural network (as described in the neural network chapter) contains an edge between each node in layer $l-1$ and each node in layer $l$.
Each edge corresponds to a unique parameter, resulting in a dense connectivity pattern and a large number of parameters. In contrast, convolutional neural networks (CNNs) use a small kernel with size $W_{\text{kernel}} \times H_{\text{kernel}}$, which is applied across the input.
This reduces the number of parameters and results in sparse interactions between input and output, since each output unit depends only on a small part of the input. Fx. CNN with kernel $3 \times 3$ and image $64 \times 64$, $\text{stride} = 1$, $\text{padding} = \text{"same"}$ then the number of connections is $\text{\#connections} = 64^23^2 = 36864$. Now, look at a fully connected neural network with input image of dim $64 \times 64$, that has  $512$ hidden units, then the number of connections is $\text{\#connections} = 64^2\cdot 512=2097152$.
This means that computing the output requires fewer operations.
\item \textbf{Parameter sharing: } In a typical fully connected neural network, all parameters are uniquely used once when computing the output of a layer. However, in a CNN, parameters are shared, meaning that weights can be used multiple times on different inputs. It follows the same growth as above in terms of computation time. So by sharing parameters, the number of computations and memory that have to be used is significantly lowered. Because of the lower number of parameters, the model needs less data to train on, because there are fewer parameters to train. Further, we see that there is less overfitting when using fewer parameters.
\item \textbf{Equivariant representations: } Equivariance between the functions $g(x)$ and $f(x)$ means that $g(f(x)) = f(g(x))$. Intuitively, this just means that no matter in which order the functions are applied, the results will be equivalent. For convolution, the type of parameter sharing makes it so the layer is equivariant to translation. Meaning translation that shifts the input, lets say $g(I)(i,j)=I(i - 1,j - 1)$ then $\left(K * g(I)\right)(i, j)=(K * I)\left(i-1, j-1\right)$.
\end{enumerate}
\subsubsection{Pooling}
The pooling operation on two-dimensional matrices pools together a region and outputs a singular value. Examples of pooling include max pooling and average pooling. Max pooling outputs the maximum value of the values included in its region (pools the whole region together to the maximum value). The average pool finds the average of the region and outputs it.
Pooling helps with invariance because it pools together a region and unifies it into one value. This leads to the fact that small variances will be smoothed out by the pooling.
When you add pooling, it's like assuming an infinitely strong prior. The prior being that the learned function is invariant to minor translations. This will heavily benefit statistical efficiency, granted the prior assumption is correct.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{images/maxpool.png}
\caption{Example of max pooling. (Adapted from \cite{maxpool_cs_wiki})} % Dummy caption generated using lipsum
\label{fig:n7}
\end{figure}
\subsubsection{Structure of CNN}
All in all, a CNN consists of a minimum of five layers: input, convolution, activation function, pooling, and an output layer.
The input and output layers are self-explanatory. The input layer is made up of several dimensions and some channels. In the case of an image, there would be two dimensions (height and width) and three channels for RGB images or one channel for greyscale images. The output layer can vary quite a bit depending on the prediction, it could be an image or several classes associated with a probability.
The convolution layer is where the model has its parameters/kernel and is, therefore, where the model learns. It detects local patterns and outputs these as hidden channels. In the convolution layer, the number of parameters depends on the number of input channels and hidden channels, as well as the size of the kernel. Imagine Input: $(C_{in}, H, W)$. Output: $(C_{out}, H_{out}, W_{out})$ for the convolution, then there is a kernel for each pair of input channel and output channel. These hidden channels are feature maps, they represent a learned feature cast down from the input channel by the associated kernel.


Earlier, we discussed why there has to be a non-linear activation function in between convolutional layers, but it is basically to preserve the complexity of the model and not just let it be boiled down to one linear model.
The pooling layer has been discussed in the above chapter; it makes the network more invariant and lessens the number of computations. Sometimes there is a fully connected layer, this layer sums up all the learned features and helps point to a conclusion.


\subsection{Convolutional LSTM}
By combining the capabilities of the LSTM and the CNN, we get the convolutional LSTM. The main structure is still the LSTM, because we are using the same Cell architecture. We are just using different operands and different dimensions. By having the inputs $x_1, \dots, x_t$, cell states $c_1,\dots, c_t$, hidden states $h_1, \dots, h_t$ and the gates $i_t, f_t, o_t$ all be 3D tensors with dimensions (Channels, width, height), we can now input spatial data into our model.
The notation and setup closely follow \cite{Shi2015ConvolutionalLN}.
To handle these new dimensions, we are using the convolution operator $(*)$. All the weights
\begin{align*}
W_{xi}, W_{xf}, W_{hc}, W_{xo} \in \mathbb{R}^{C_{hidden} \times C_{input} \times kH \times kW}\\
W_{hi}, W_{hf}, W_{hc}, W_{ho} \in \mathbb{R}^{C_{hidden} \times C_{hidden} \times kH \times kW}
\end{align*}
are kernels of 4 dimensions, whereas the weight
\begin{align*}
W_{ci}, W_{cf}, W_{co}, \in \mathbb{R}^{C_{hidden} \times H \times W}
\end{align*}
is a parameter tensor and
\begin{align*}
 b_i, b_f, b_c, b_o \in \mathbb{R}^{C_{hidden}}
\end{align*}
is a bias vector.
Here we still use $\odot$ as the symbol for the Hadamard product.
\begin{align}
i_t &= \sigma(W_{xi} * \mathcal{X}_t + W_{hi} * \mathcal{H}_{t-1} + W_{ci} \odot \mathcal{C}_{t-1} + b_i) \\
f_t &= \sigma(W_{xf} * \mathcal{X}_t + W_{hf} * \mathcal{H}_{t-1} + W_{cf} \odot \mathcal{C}_{t-1} + b_f) \\
\mathcal{C}_t &= f_t \odot \mathcal{C}_{t-1} + i_t \odot \tanh(W_{xc} * \mathcal{X}_t + W_{hc} * \mathcal{H}_{t-1} + b_c) \\
o_t &= \sigma(W_{xo} * \mathcal{X}_t + W_{ho} * \mathcal{H}_{t-1} + W_{co} \odot \mathcal{C}_t + b_o) \\
\mathcal{H}_t &= o_t \odot \tanh(\mathcal{C}_t)
\end{align}
As we can see, the parameters are reused across all time frames; only the gates, hidden state, and cell state change over time. In practice, the parameters $W_{ci}, W_{cf}, W_{co}$ are just vectors of hidden channels length, which are then broadcasted up so each scalar now is a $height \times width$ matrix.
The gates are further changed by the peephole connections with the peephole weigths $W_{ci}, W_{cf}, W_{co}$, we have $W_{ci} \odot \mathcal{C}_{t-1}, W_{cf} \odot \mathcal{C}_{t-1}, W_{co} \odot \mathcal{C}_t$. We are including a term with the previous cell state, so the gates can change according to what is currently in memory. However, the peephole implementation isn't a must-do, and one should be aware that it adds weight and therefore computation time.



\section{Methodology}
This study uses a ConvLSTM model to predict the future growth of mycelium. The methods include extensive data preprocessing, choice of model architecture and hyperparameters, training with a combined loss function with, as well as an evaluation based on visual inspection and loss statistics for prediction precision.

\subsection{Data}

\subsubsection{Data collection}
The dataset \cite{andyg} was found on the open-source platform Figshare, where it was published by Andy Goldsmith. The dataset was collected over six different trials, for each trial, they grew three batches, each captured with its own camera. The image collection was done by having the yeast grow on agar plates with growth medium Yeast Extract-Peptone-Glycerol. The agar plates were organized into grids, to save space and streamline the documentation process, they were placed inside an incubator to prevent contamination of the mycelium. The images were captured with a camera placed on a motorized cart on a track inside the incubator. Photos were taken at a frequency rate that is meant to capture the critical parts of mycelium growth behavior, which means the time between each frame is in the span of hours, each colony grew over several days, most frequently around three days.

\subsubsection{Data organization}
The dataset consists of images of the yeast \textit{Saccharomyces cerevisiae}, which is the common yeast used in brewing. It is time series data, so each image represents a frame in a time-lapse of a yeast colony growing. There are 5500 colonies, 196 unique strains of the yeast species, and in total, 933103 images exist in the dataset. The image file names are named in a manner so it's easy to identify which strain and experiment iteration it belongs to; further, the growth time for the yeast at the time of capture is recorded. File names are of the form: "YPG11407\_001\_1080.jpg", YPG is the growth medium, 11407 is the strain, 001 is the experiment iteration, and 1080 is for how long it has been growing in minutes.

\subsubsection{Data preprocessing}
There were a couple problems identified with how the data were organized, when loading in data, no meta data is describing the order of the images in the directory (which was made so frames was in chronological order etc.), when loading in the files the Pytorch program therefore went to the default and sorted files alphabetically, which is a problem because it would then sort images at time 1200 before images at time 200.
So we wrote a program that stores the strain, experiment iteration, and time frame, and then sorts each frame numerically from lowest to highest instead of alphabetically. Another problem we ran into was that because we are using the convolution operator, all the image resolutions had to be the same, which they were not, since each image is cut to the edge of the hyphae expansion, and since the hyphae expand for each image, they all had different resolutions.
We found the highest width and height in the data and then padded all the images with black pixels so they all had the resolution $\text{Max Width} \lambda_{\text{scale}} \times \text{Max Height} \lambda_{\text{scale}}$, depending on the scale factor of the images. We padded them in a way that kept the original image centered. Then we grouped the images into sets of strain and experiment iteration so that the frames are sorted as a sequence.
We then split each sequence into input frames and target frames. The same number of frames that needs to be predicted, the same number of target frames is needed for training. Say we want to predict $80$ frames, then $80$ target frames will be stored and removed from the sequence, and the remaining sequence is then the input sequence.


To be able to train the model, we had to make sure that each batch of sequences had the same number of frames. This is already fulfilled for the target sequence, but the input sequence will have varying lengths. Therefore, we also padded the input sequences by finding the maximum number of frames for the input sequences in a batch and then added black images at the end of the rest of the input sequences in that batch, so the lengths match.


The entire dataset is split up into three segments, a training set, a validation set, and a test set, each consisting of $70\%$, $15\%$, and $15\%$. This part is crucial when it comes to the methodology of Machine Learning. Since we are using data to train the model, that data cannot be used to evaluate the model because the model is fitted to that exact data, so the error will always be low, granted you made a good model. The only real way to test if a model is good is to validate it on some data that it has not been trained on, this is known as cross-validation. Therefore, we keep a segment of the data as validation data and keep checking our produced models on that validation data, to make sure we are not overfitting, and it captures the underlying model that distributes this data. 
One has to be careful because once some data has been used for validation and we change parameters according to that validation process, the model now becomes biased towards that data, and the model and the validation data are no longer independent. That's why we keep a portion for testing, this is the part of the dataset that has to be locked away while models are being created, because once we use it for testing, we can no longer make changes to the model and make an unbiased evaluation of the model using that data. We are essentially burning data every time we use it to test the model's ability to capture the underlying structure of the data. The more we change our models based on the errors we see in the validation set, the more biased that data and our models become, which is not good, because ideally, we want the model to be as unbiased as possible so we know how it will perform on new data.

\subsection{Reasoning behind choices}
\subsubsection{ConvLSTM}
In this study, we use the ConvLSTM model. The reasoning behind this is simple:
\begin{enumerate}
 \item The model is capable of learning spatiotemporal patterns. It can do so locally, which is great for mycelium data since it only spreads to neighbouring pixels.
 \item It is combining the strengths of two basic strong deep learning methods. Due to the capabilities of the LSTM, it processes sequences chronologically by having hidden states over time, which is advantageous because growth happens step by step. CNN is considered one of the strongest methods for image analysis. So the combination of these models is rather sensical.
 \item ConvLSTM is super memory efficient compared to other spatiotemporal models. LSTM conserves weights over time because it's recurrent. Thanks to the convolutional operator, it is possible to retain the native data structure and use a small number of weights for the kernel. ConvLSTM can do step-wise prediction, meaning it can generate one frame at a time, each based on the previous hidden state, this is known as an autoregressive model. Each predicted frame is then collated into a sequence, this is in contrast to predicting the entire sequence at once. This makes it so that longer sequences can be predicted without worrying about running out of memory.
\end{enumerate}

\subsubsection{Backpropagation}
The choice of backpropagation was straightforward.
\begin{enumerate}
 \item It is efficient in calculating the gradients with respect to the parameters.
 \item It works well with gradient descent.
 \item Scales well and works for all computation graphs.
 \item It is the most widely implemented algorithm in deep learning libraries.
\end{enumerate}

\subsubsection{Gradient descent: ADAM}
Considerations with gradient descent
\begin{enumerate}
 \item It is the most implemented method
 \item The method is simple, effective and scales well.
\end{enumerate}
Reasoning behind ADAM
\begin{enumerate}
 \item Has momentum which is great, because it remembers earlier gradients, which smooths out updates.
 \item Works well for sequences and mini-batches.
\end{enumerate}

\subsubsection{Loss functions: Dice and BCE}
BCE with a sigmoid layer is a relatively normal loss function to use for binary segmentation, which is much like what we are doing.
\begin{enumerate}
 \item It's great at measuring each pixel and for comparing the probability prediction to the target. It does so in a way that strongly directs the loss by punishing wrong predictions hard.
 \item Works directly on what we are doing, which is predicting a binary probability for each pixel in the image.
\end{enumerate}
The Dice loss is a mathematical translation of what we are trying to do, which is to make the mycelium predictions overlap the target.
\begin{enumerate}
 \item Dice loss is good for data with a disproportionate background and foreground ratio. It only focuses on the overlap between the foreground of the target and the prediction.
\end{enumerate}
By combining these losses, we get the best of both worlds. The BCE, which is focusing hard on correct predictions, and the dice loss, which is more forgiving and works well for disproportionate datasets. Both losses use sigmoid activation so the outputs can be interpreted as probabilities.

\subsection{Software and tools}
To implement the model and load, sort, transform, and plot data, the programming language Python v.3.12.2 was installed through conda-forge in a conda environment.
For implementation, we are using the following libraries
\begin{itemize}
 \item \textbf{Standard Python libraries:} Operating system interface (OS), Random, multiprocessing.dummy
 \item \textbf{Data handling:} Numerical Python (numpy), Python data analysis (Pandas)
 \item \textbf{Plotting and visualization:} Matplotlib.pyplot
 \item \textbf{Image and videos:} OpenCV (cv2), Pillow (PIL, Image, ImageOps)
 \item \textbf{Progress bar:} Taqaddum (tqdm)
 \item \textbf{Pytorch:} torch, torch.nn, torch.nn.functional, torch.optim, torch.utils.data,
 
  torch.cuda.amp
 \item \textbf{TorchVision:} torchvision.transforms, torchvision.datasets
 \item \textbf{Scientific computing:} Scientific Python
  
 (SciPy, scipy.ndimage.distance\_transform\_edt)
 \item \textbf{Image morphology:} Scikit-image morphology module (skimage.morphology)
\end{itemize}


\subsection{Hardware}
Data manipulation, model creation, and training happen in Python and Jupyter notebook files. For testing and visualization, Jupyter Notebook is used; for training, we run the Python file from the terminal on a cloud computer.
The training happens on a cloud computer running an AMD EPYC 9454 CPU with 48 physical cores / 192 threads and 2.75 GHz, combined with a NVIDIA H100 GPU and 756 GiB RAM. This is an incredibly strong computer, the GPU is the state of the art when it comes to training AI models, and it has the RAM and CPU to support it. The reason this is so important is that AI models can be trained using parallel operations on GPU's which makes the training process 10x-100x faster.
With a dataset of this magnitude and a rather memory-hungry model, training on a regular computer is not considered viable. The test runs and data manipulation, as well as visualization, are done on a regular computer, however.

\subsection{Training}
When we say that a model learns a pattern, it comes from the notion that we have a training loop, where the model is step-wise optimized and thereby learning.
When starting the training, the model parameters are initialized, a gradient descent optimizer and a loss function are chosen. In our case the optimizer is ADAM and the loss function is 
\begin{align}
L(\theta; x^{(j)}; y^{(j)}) = 0.6 \cdot \text{Focal weight} \cdot L_{\text{Weighted BCE with sigmoid layer}}+ 0.4 \cdot L_{\text {dice }}
\end{align}
The loss function implementation is included in Appendix~\hyperlink{sec:loss}{Loss Function}

Normally, in a training loop, there is an iterator that repeats the training process for some epochs, so for each epoch, the model is trained on the entire training dataset.
The training data is split into batches, where the batch size is the number of samples in each batch. For each batch, a forward pass happens, the model makes predictions based on the batch, and the loss is computed on the predictions.
Then the gradient of the loss function with respect to the parameters is computed; in our case, gradients are computed using backpropagation, and then the parameters are changed according to the gradient descent algorithm.
This is known as a backward pass. After each backward pass, the training loss is logged for later evaluation. After each epoch, the loss is found according to the achieved parameters, on the validation set.
If the validation loss is better than any previous validation losses, then the parameters are saved.

The implemented training loop is included in Appendix~\hyperlink{sec:training}{Training loop}


\section{Experiments and analysis}
\subsection{Model}
This is the Architecture of each cell in the implemented ConvLSTM model:
\begin{align}
i_t &= \sigma(W_{i} * [\mathcal{X}_t, \mathcal{H}_{t-1}]) \\
f_t &= \sigma(W_{f} * [\mathcal{X}_t, \mathcal{H}_{t-1}]) \\
\hat{\mathcal{C}_t} &= \tanh(W_{c} * [\mathcal{X}_t, \mathcal{H}_{t-1}])\\
\mathcal{C}_t &= f_t \odot \mathcal{C}_{t-1} + i_t \odot \hat{\mathcal{C}_t} \\
o_t &= \sigma(W_{o} * [\mathcal{X}_t, \mathcal{H}_{t-1}]) \\
\mathcal{H}_t &= o_t \odot \tanh(\mathcal{C}_t)
\end{align}
As we can see, it has no bias weights and no peephole connections.
There is also a difference in the input, we concatenate $\mathcal{X}_t$ and $\mathcal{H}_{t-1}$ a long the channel dimension and get this: $[\mathcal{X}_t, \mathcal{H}_{t-1}]$. $\mathcal{X}_t$ and $\mathcal{H}_{t-1}$ has shape $(\text{batches}, C_{\text{hidden channels}}, H, W)$ so when we concatenate $[\mathcal{X}_t, \mathcal{H}_{t-1}]$. $\mathcal{X}_t$ has shape $(\text{batches}, C_{\text{hidden channels}}+C_{\text{hidden channels}}, H, W)$.
This is more efficient because we only need one convolution per gate; it simplifies the code, and the end result is the same.




The cell architecture as implemented in Python
\begin{lstlisting}
  class ConvLSTMCell(nn.Module):
  def __init__(self, hidden_channels, kernel_size):
      super(ConvLSTMCell, self).__init__()
      self.hidden_channels = hidden_channels
      self.kernel_size = kernel_size




      #Convolution LSTM gates
      self.conv_i = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)
      self.conv_f = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)
      self.conv_o = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)
      self.conv_c = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)




  def forward(self, x, h_prev, c_prev):
      combined = torch.cat([x, h_prev], dim=1)  #Concatenate
      i_t = torch.sigmoid(self.conv_i(combined))
      f_t = torch.sigmoid(self.conv_f(combined))
      o_t = torch.sigmoid(self.conv_o(combined))
      c_t_hat = torch.tanh(self.conv_c(combined))
      c_t = f_t * c_prev + i_t * c_t_hat
      h_t = o_t * torch.tanh(c_t)
      return h_t, c_t
\end{lstlisting}
Each gate is a convolutional layer followed by an activation function. This cell is called in the implementation of the full ConvLSTM model.




The full model combines the cells in a meaningful manner, which in this case is a chronological one, so it follows the theory behind the ConvLSTM, but also so it is capable of solving the problem of this thesis. The solution is a greyscale image prediction sequence of the mycelium with constant length.
The model takes several arguments, which are parameters that decide the depth, width, etc. of the model. In the forward, where each component of the model is called, the model is assembled. The pipeline is as follows:
The model is fed the input sequence, each frame is processed chronologically. First, the channel of the frame is sent to the input\_adapter, where it is broadcast to several hidden channels, so the cell can process it. These hidden channels are then used as input to the cell. The cell, based on that input and the cell state, computes another number of hidden channels and an updated cell state. For each $layer > 1$, the computed hidden channels and cell state are then used again for input to the cell. This is done for each frame in the input sequence.
When all the frames in the input sequence have been processed, the last hidden channels and the last cell state are used to make predictions. When making predictions, the same thing happens, except the cell is not fed the input sequence, but instead just the last hidden channels for each layer and frame. Then, for each frame, the output\_adapter is called, it takes the hidden channels as input and reduces them to one channel representing the greyscale channel, this is the result of the output\_adapter: $(B, \text{Hidden channels}, H, W) \rightarrow (B, 1, H, W)$.
\begin{lstlisting}
  class ConvLSTM(nn.Module):
  def __init__(self, input_channels, hidden_channels, kernel_size, num_layers):
      super(ConvLSTM, self).__init__()
      self.hidden_channels = hidden_channels
      self.num_layers = num_layers




      #Project input_channels -> hidden_channels
      self.input_adapter = nn.Conv2d(input_channels, hidden_channels, kernel_size=1)




      #Project hidden_channels -> output channel (1 for grayscale prediction)
      self.output_adapter = nn.Conv2d(hidden_channels, 1, kernel_size=1)




      #Stack ConvLSTM cells (same input/output channels)
      self.cells = nn.ModuleList([
          ConvLSTMCell(hidden_channels, kernel_size)
          for _ in range(num_layers)
      ])




  def init_hidden(self, batch_size, spatial_size):
      H, W = spatial_size
      device = next(self.parameters()).device
      h = [torch.zeros(batch_size, self.hidden_channels, H, W, device=device) for _ in range(self.num_layers)]
      c = [torch.zeros(batch_size, self.hidden_channels, H, W, device=device) for _ in range(self.num_layers)]
      return h, c




  def forward(self, input_sequence: torch.Tensor, future: int = 0) -> torch.Tensor:
      B, T_in, _, H, W = input_sequence.size()
      h, c = self.init_hidden(B, (H, W))
    
      #Process input sequence to update hidden state
      for t in range(T_in):
          input_t = self.input_adapter(input_sequence[:, t])
          for i in range(self.num_layers):
              cell_input = input_t if i == 0 else h[i - 1]
              h[i], c[i] = self.cells[i](cell_input, h[i].detach(), c[i].detach())




       outputs = []
      input_t = h[-1]
       for t in range(future):
          for i in range(self.num_layers):
              cell_input = input_t if i == 0 else h[i - 1]
              h[i], c[i] = self.cells[i](cell_input, h[i], c[i])
          input_t = h[-1]
          output_frame = self.output_adapter(input_t)
          outputs.append(output_frame)
       return torch.stack(outputs, dim=1)  #[B, T_future, 1, H, W]
\end{lstlisting}
The number of predictions depends on the value of the integer $T\_\text{future}$. The outputs $[B, T\_\text{future}, 1, H, W]$ will typically not have values in $[0,1]$ as for greyscale, this is because of the sigmoid layer in the loss function.


\subsubsection{Model architecture}
The model can be constructed in many different sensical ways. The models for this project will have $1$ to $3$ layers. Each layer will consist of a ConvLSTM cell with the convolution layer gates. The model utilizes two types of $1 \times $ convolutional projection layers, the output\_adapter and the input\_adapter, made for projecting back and forth between hidden channels and input channels. This is what the core structure of the model looks like.


For the thesis, several ConvLSTM models have been constructed with varying hyperparameters. The kernel size is constant for all models and is set at $3 \times 3$. All greyscale image tensors have to have the same height and width dimensions for the model to process them. The max height is $371$ and the max width is $408$, images of this size are considered big. In order to combat the size issue, a scale factor parameter is added that scales all the images down. This dramatically reduces the amount of padding that has to be done and the memory it takes to load the data.
The input channel for all the data is greyscale, which sets $C_{in} = 1$. The length of the input sequence is calculated based on the length of the output sequence, $T_{in} = \text{length}(\text{sequence}) - T\_\text{future}$. The number of batches loaded in at once depends on the size of the model; if the model is wide or deep, the batch size has to be smaller, so the memory usage equals out. For $1$ and $2$ layer models, the batch size is $B=10$, for $3$ layer deep models with $\text{hidden channels}<64$ we have $B=5$, for the model with $3$ layers and $64$ hidden channels $b=3$. The input dimensions are then $[B, T_{in}, C_{in}=1, H=371\lambda_{scale}, W=408\lambda_{scale}]$.
The number of predicted frames/length of the output is a constant variable $T\_\text{future}$. Experiments include models with $T\_\text{future} = 20$, $T\_\text{future} = 50$, $T\_\text{future} = 80$ and $T\_\text{future} = 100$. The model's architecture is autoregressive, which means each predicted frame is fed into the next time step. This is opposed to multiframe prediction, where multiple frames are used. In addition, the model is also unidirectional, meaning there is no backward pass over time; it only goes forward. The activation functions are consistent with the traditional ConvLSTM structure.




\subsection{Data optimization}
Data optimization is crucial for good results in machine learning. The models are trained on the data and learns the patterns that it can find in the data, so it minimizes the loss. The problem for this thesis is to predict the growth of a mycelium network, therefore, the data should reflect that network growth in a way that is sensical for the prediction.
It is evident from the raw images, in Figure~\ref{fig:n11}, that there is some noise, and the small nuances of white make it difficult to distinguish between the mycelium hyphae/root network and the non-hyphae white background region, on which the main hyphae grow.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/raw_target.png}
\caption{Visualization of the last $4$ frames in a non-manipulated image sequence.} % Dummy caption generated using lipsum
\label{fig:n11}
\end{figure}




A decision has been made to binarize the data by using adaptive thresholding and then skeletonize using the algorithm in T. Y. Zhang and C. Y. Suen's paper \cite{10.1145/357994.358023}. To use this algorithm successfully, the data must be altered. Firstly data is converted from $[C_{in}, H, W]$ to $[H, W]$. Image is then cleaned by setting all $values \leq 0.2$ to be equal to $0$. The cleaned image is then processed by Gaussian blur, which applies a Gaussian kernel across the image, blurring the image out more, making it less variant. Each entrance in the cleaned image is then divided by each entrance in the Gaussian blur image $R(i, j) = \frac{I(i, j)}{\text{GaussianBlur}(I)(i, j) + \epsilon}$.
The Gaussian blur captures the illumination difference on the images and, by dividing by it, attempts to smooth out this difference. Values are then normalized to fit greyscale in the range $[0, 1]$. Now, to highlight the hyphae, Contrast Limited Adaptive Histogram Equalization (CLAHE) is applied to the image. CLAHE in this thesis is used as a black box. Briefly explained, it amplifies contrasts locally, which is good for faint hyphae and hyphae in varying lighting. The altered image is then binarized using adaptive thresholding, which has to be adaptive because of the variable lighting and possible difference in concentration of hyphae. It works like this: $\text{binary}(i,j) =
\begin{cases}
1 & \text{if } I(i,j) > \text{mean}(N_{i,j}) \\
0 & \text{otherwise}
\end{cases}$, it finds the mean of a local region and then zeroes out all entrances with $values \leq \text{mean}(N_{i,j})$. To fix holes and gaps in the binary image, we use the \href{https://github.com/scikit-image/scikit-image/blob/v0.25.2/skimage/morphology/gray.py#L443-L517}{Closing} function, which is described in detail in \cite{scikitimage2025gray}. The function is treated as a black box, it serves the purpose of making the hyphae continuous and the mycelium network connected. The next step is to skeletonize the binary image so that each hyphae is of the same thickness. At last, the image is transformed by \href{https://github.com/scikit-image/scikit-image/blob/v0.25.2/skimage/morphology/gray.py#L245-L363}{Dilation}, which makes the skeleton thicker. The function is described in the SciKit GitHub repository \cite{scikitimage2025gray_dilation}. The goal of this is to balance the proportions of foreground and background pixels.
The image is then resized according to the scale factor $\lambda_{scale}$ and then padded to be of size $(\text{height}_{max}\lambda_{scale} \times \text{width}_{max}\lambda_{scale})$. The skeletonization/optimization pipeline is included in Appendix~\hyperlink{sec:optim}{Data skeletonization pipeline}.
The effect of the data skeletonization pipeline is showcased in Figure~\ref{fig:n12}, where the images in Figure~\ref{fig:n11} have been processed by said pipeline.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/skeleton_target.png}
\caption{Visualization of the last $4$ frames in an optimized image sequence. $\lambda_{scale}=0.5$, dilation with disk(1).} % Dummy caption generated using lipsum
\label{fig:n12}
\end{figure}




To tackle the problem of sparse foreground pixels, further measures have to be taken. One of the factors for the low proportion of foreground pixels stems from the data optimization pipeline, where we remove a big chunk of what was considered foreground initially. However, the main factor is the max size padding, which adds all around a lot of extra background. The amount of padding is, of course, dependent on each sample, but based on visual inspections, it seems like a good chunk of the data is quite a bit smaller than the max size. The skeleton has already been dilated, which is a good first step. Secondly, we want to find a mask that masks the relevant part of the image. This mask can then be passed to the loss functions, which can then be computed on only the masked area.
The mask is found by setting a threshold, if $value>0$ then $value = 1$ else $value = 0$. Since all $padding = 0$ it will not be a part of the mask; one could argue that there could be some relevant background which has $value = 0$, but this is assumed to be very unlikely since the raw data images have not been cleaned.
The masks associated with the images in Figure~\ref{fig:n12} are displayed in Figure~\ref{fig:n13}.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/mask_target.png}
\caption{Visualization of the last $4$ masks for the frames in an image sequence.} % Dummy caption generated using lipsum
\label{fig:n13}
\end{figure}




After all the data optimization, the foreground percentage with and without the mask, across the entire target training set, is as follows:


For $\lambda_{scale} = 0.5$ and no dilation
\begin{align*}
\frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 24.12\%\\
\frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 5.61\%
\end{align*}
For $\lambda_{scale} = 0.5$ and dilation with disk$(1)$
\begin{align*}
\frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 36.31\%\\
\frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 8.44\%
\end{align*}
This shows a much healthier percentage of foreground, which implies a more balanced dataset. The settings $\lambda_{scale} = 0.5$ and no dilation are the closest to $25\%$ foreground and $75\%$ background, which is the desired ratio. Just because dilation is not needed when $\lambda_{scale}=0.5$, doesn't mean it is not useful; with $\lambda_{scale}=1$, dilation is much needed to balance the dataset out.




At first, we processed the images with dilation after the resizing, which made the data especially vulnerable to changes in the scale factor. This meant that the foreground would become too dominant; the resulting percentages were:


For $\lambda_{scale} = 0.5$ and post resizing dilation, with disk$(1)$
\begin{align*}
\frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 45.63\%\\
\frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 10.61\%
\end{align*}
For $\lambda_{scale} = 0.5$ and post resizing dilation with disk$(2)$
\begin{align*}
\frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 60.12\%\\
\frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 14\%
\end{align*}
Since the forecast horizon can be rather long $T\_\text{future} = 20, 40, 80$, it is necessary to make sure the remaining frames for the input are long enough to be meaningful. The minimum length for input sequences is set at $20$ frames; this is still rather short, but depending on the sequence, it could be long enough to be meaningful.




\subsection{Evaluation}
\subsubsection{Foreground-to-background proportion}
To test the effect of pixel foreground and background ratios on ConvLSTM, we run three models respectively with $1$, $2$, and $3$ layers. They are run on the three datasets proposed in the Data optimization section, it is the datasets with $45.63\%$, $36.31\%$, and $24.12\%$ foreground. They will be referred to as dataset 1, dataset 2, and dataset 3.
For each model evaluated throughout this section, there is an associated prediction sequence, a table with loss statistics, and a text file with all hyperparameters listed. The information is available at \cite{gagarahn2025bachelor}, and each model can uniquely be identified by the number of layers, hidden channels, prediction frames, and training data foreground percentage.


The first model that is run is the ConvLSTM as described in the Model section, it is run with $1$ layer, $32$ hidden channels, and $T\_\text{future} = 80$. Figure~\ref{fig:n14} showcases the first frame in its prediction sequence next to the ground truth target image.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/bfcomp1.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n14}
\end{figure}
The red square is the mask that is applied to the loss function, this means that only the prediction contained within that red square is considered, both in regards to evaluation and training.
Visually, when looking at Figure~\ref{fig:n14}, the model seems to capture the characteristics of the mycelium pattern, but it does so with varying conviction. On dataset 1, the gaps between the hyphae are not as distinct as with the other two, this is interpreted as the prediction being less sure of where the hyphae are. I think the prediction on datasets 2 and 3 is comparable to each other; both capture features rather well and seem to do so with the same conviction. Looking at the validation data in Table~\ref{tab:1}, it is evident that dataset $1$ has the lowest validation loss and the highest dice score.
\begin{table}[H]
\centering
\caption{Model Comparison: (1 Layer)}
\label{tab:1}
\begin{tabular}{|c|c|c|c|}
\hline
Metric & 45.63\% & 36.31\% & 24.12\% \\
\hline
Train Loss & 0.1813 & 0.2138 & 0.2608 \\
Val Loss   & 0.1899 & 0.2221 & 0.2649 \\
Dice Loss  & 0.1699 & 0.1979 & 0.2490 \\
\hline
\end{tabular}
\end{table}
The next model has $2$ layers, $32$ hidden channels, and $T\_\text{future} = 80$, it is a slightly deeper model than the one before. Right out of the gate, looking at Figure~\ref{fig:n15}, it is seen that the model is likely underfitting the data. This is typical for a high foreground percentage because it's more likely to be correct if guessing over a broad amount of pixels than if the image had a low foreground percentage. On the other hand, the model seems to be fitting quite well to datasets $2$ and $3$, when comparing ground truth to predictions. The background is a dark grey, meaning it isn't averaging its guesses, but tries to capture the pattern of the mycelium. It seems like the contrast between background and foreground is the highest on dataset $3$, which indicates a more confident model.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/bfcomp2.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n15}
\end{figure}
As presented in Table~\ref{tab:2}, the validation loss for the model is lower on dataset $1$ than on the others. One of the reasons why it might be lower than dataset $3$'s model is that there is more foreground to be corrected on. An explanation is that by guessing uniformly, there is a higher chance of being semi-correct, and since confidently wrong guesses are punished harder due to BCE, it has a lower loss.




\begin{table}[H]
\centering
\caption{Model Comparison: (2 Layers)}
\label{tab:2}
\begin{tabular}{|c|c|c|c|}
\hline
Metric & 45.63\% & 36.31\% & 24.12\% \\
\hline
Train Loss & 0.2119 & 0.2095 & 0.2594 \\
Val Loss   & 0.2127 & 0.2170 & 0.2635 \\
Dice Loss  & 0.1844 & 0.1875 & 0.2433 \\
\hline
\end{tabular}
\end{table}
The third model with $3$ layers, $32$ hidden channels, and $T\_\text{future} = 80$ has more varying results. Observing Figure~\ref{fig:n16}, it looks like the model is underfitting on data $1$ and therefore is capturing none of the complexity. The model trained on dataset 2 seems to be struggling with confidently predicting the hyphae. This is likely due to small gaps between the hyphae and the high foreground density. Visually, the model trained on dataset $3$ looks superior, it mimics the ground truth mycelium pattern and might be "anticipating" the growth of the mycelium in future frames by having a faded border around the mycelium.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/bfcomp3.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n16}
\end{figure}
However, according to Table~\ref{tab:3}, the validation loss is lower and the dice score is higher for the models trained on higher density foreground datasets.
\begin{table}[H]
\centering
\caption{Model Comparison: (3 Layers)}
\label{tab:3}
\begin{tabular}{|c|c|c|c|}
\hline
Metric & 45.63\% & 36.31\% & 24.12\% \\
\hline
Train Loss & 0.2127 & 0.2061 & 0.2542 \\
Val Loss   & 0.2133 & 0.2172 & 0.2623 \\
Dice Loss  & 0.1844 & 0.1827 & 0.2348 \\
\hline
\end{tabular}
\end{table}
All future models are trained on dataset 3, meaning the dataset with no dilation and a foreground percentage of $24.12\%$.
\subsubsection{Depth}
In this section, we will be examining three models, each with varying depths, The three models have $1$, $2$, and $3$ layers respectively. The models are identical in all other regards, they have 32 hidden channels, the same loss function, and prediction length. Since the model with three layers uses more memory, we had to decrease the batch size to $5$ for the training of that specific model. This number of layers has been chosen because biological skeletons usually don't require deep models to learn, and there is also a memory restriction on the hardware used for training the models. We will be examining the initial frame prediction and a thresholded binarized version of the prediction compared to the target. Further, we will be discussing how the prediction sequence changes as it progresses in time; all sequences can be found at \cite{gagarahn2025bachelor} as GIF files.




It looks like the first $1$ layer model made a prediction that is a decent fit to the target, as you can tell by Figure~\ref{fig:n17}. You can tell the prediction is a bit fuzzy, and observing the thresholded prediction, it's evident that the predicted hyphae are quite a bit thicker than those of the target. The entire predicted sequence is available as a GIF file in the GitHub repository associated with this thesis \cite{gagarahn2025bachelor}.
I would say the predicted sequence even performs decently in the later frames, it is not identical to the target, but it is predicting that the mycelium keeps expanding. It seems to be unable to single out any new pattern of the growing hyphae, but instead just makes a solid, rather uniform prediction around the established pattern.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L1_H32_T80.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n17}
\end{figure}
The prediction sample in Figure~\ref{fig:n18}, which was produced by the two-layer model, initially seems visually worse than the prediction by the $1$ layer model, due to how the thresholded version looks. However, it seems to be catching some details that the other model didn't. It has been learned that the mycelium grows from the center, which creates a circular growth pattern. We can tell this by the grey cloud around the complex pattern. This is, however, the only trait it is keeping as the sequence goes on, because it is forgetting the intricate net-like pattern of the hyphae. Instead, it fades into a grey cloud, and only by observing the threshold can you see that it is keeping its circular prediction.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L2_H32_T80v1.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n18}
\end{figure}
The thresholded image of the prediction shown in Figure~\ref{fig:n19}, made by the $3$ layer model, looks close to that of the target, although it has a bit thicker hyphae. When just looking at the raw prediction, a few details stand out. The model has predicted that the future frames are gonna be circular and close around the initial mycelium net. Also, the background is darker than that of the prediction in Figure~\ref{fig:n17}, which means it is more confident that there won't be any foreground there. Looking at the GIF for the entire sequence, we observe that it also struggles from blurring into a grey cloud, however, the net-like pattern is still visible, just more faint. Examining the thresholded version, one can see that it is not just blurring completely out; it still maintains a decent prediction for what should be considered foreground. Not optimal for pattern recognition, but it does predict accurately that it grows and the way it grows, although in the end, it overpredicts how much growth will happen.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L3_H32_T80.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n19}
\end{figure}
The two models that stand out, when doing the visual evaluation, are the ones with $1$ layer and $3$ layers.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/comparison_loss_plot_depth.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n20}
\end{figure}
Looking at Figure~\ref{fig:n20}, it is observed that the model with $3$ layers reaches the lowest minimum validation loss and also the lowest test and dice loss compared to the two other models. Based on the visual evaluation and the loss function statistics, the $3$ layer model is the one we will use for further research.
\subsubsection{Width}
In this section, we will be examining three predictions made by three models with varying widths in the hidden channels, further, we will be looking at the loss statistics for the three models. It is models with $3$ layers and $8$, $16$, and $64$ hidden layers all predicting sequences of length $80$, which are trained and evaluated on the non-dilation dataset.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L3_H8_T80.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n21}
\end{figure}
According to Figure~\ref{fig:n21}, the $8$ hidden channel modelâs prediction is not capturing any patterns or complexities; it is making a uniform guess, which suggests underfitting. The claim is further supported by the loss statistics in Figure~\ref{fig:n24}. $8$ hidden channels are considered shallow for a neural network, and it might not be enough to capture the complexities of mycelium networks.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L3_H16_T80.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n22}
\end{figure}
The prediction, in Figure~\ref{fig:n22}, made by the $16$ hidden channels model is blurry initially, but manages to keep its form and rather accurately depicts the growth of the target. This is especially true for the threshold version of the prediction. It keeps the separations until a pretty late stage, and the outline is not far off from the target. Overall, the prediction suggests that the model is not as good at making as accurate predictions as the others early on, but is more stable over time.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L3_H64_T80.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n23}
\end{figure}
Examining Figure~\ref{fig:n23}, showcasing the prediction made by the $64$ hidden channels model, it can be seen that the model is confident in its prediction. There is high contrast between the hyphae and the background, you can even see what we assume to be fainter hyphae, being less white because they will only be included in the target some of the time, due to the optimization pipeline. The mycelium net is not fuzzy, and the threshold is seemingly accurate to the target, except for some extended hyphae which show up later in the target sequence.
The model seems to preserve its complexity better than any previous model, the threshold still has the distinction between foreground and background in the end. It is still struggling with blurring out in later frames and also overextending its spread/growth.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/comparison_loss_plot_width.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n24}
\end{figure}
According to the loss statistics in Figure~\ref{fig:n24}, the model with $16$ hidden channels performed comparable to the one with $32$ hidden channels, and visually they also express the same behaviour. The $64$ hidden channels model outperforms all previous models when it comes to loss statistics. Visually, it is also doing the best or the same as the best models we have looked at so far.
\subsubsection{Forecast horizon}
In this section, we will be evaluating how effective the model with $3 $ layers and $64$ hidden channels is on different prediction sequence lengths. The models are trained to predict $20$, $50$, and $100$ frames ahead.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L3_H64_T20.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n25}
\end{figure}
Figure~\ref{fig:n25} featuring the $20$ frames long prediction made by the model trained with a forecast horizon of $20$. This is the most accurate prediction our model has produced. The first frame in the sequence is very close to the target frame, and when applying the threshold, they look almost the same.
When inspecting the sequence in the GitHub repository, it is seen that the model is predicting the mycelium net to expand; it is unable to predict the distinct hyphae, but just predicts a general expansion around the circular net. The raw prediction is diluting details, but the complexities of the mycelium pattern are still clearly visible.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L3_H64_T50.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n26}
\end{figure}
The $50$ frames long prediction sequence is not showcasing any learned complexities or patterns as evident from Figure~\ref{fig:n26}. It is comparable to the model with $8$ hidden channels and the predictions made on the data with a higher foreground percentage. It is a sign of underfitting, and this claim of underfitting is also supported by the loss statistic in Figure~\ref{fig:n28}. It is rather an odd occurrence, since shorter sequences are expected to outperform longer sequences. I did train the model twice with these settings and on the same data, and it got stuck in the same local minimum twice.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/L3_H64_T100.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n27}
\end{figure}
For the prediction sequence with $100$ frames, a not-too-unexpected behavior is apparent from Figure~\ref{fig:n27} and the GIF sequence. The predicted growth of the mycelium is not as complex as the target; it is, however, pretty close in size and shape. One could interpret this as the model minimizing loss by hedging its bet that the mycelium will expand in a circular pattern and not guessing the pattern of the hyphae.
\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{images/comparison_loss_plot_time.png}
\caption{} % Dummy caption generated using lipsum
\label{fig:n28}
\end{figure}
Inspecting Figure~\ref{fig:n28}, we can see that the $100$ frames model performed better than expected in regards to the loss statistics. Visually, it is not useful for predicting patterns, but predicting the expected covered area it seemed to do well. Another rather unexpected finding was that the $50$ frames model underperformed to such a degree, performing the worst out of all models. The $20$ frame model performed, to no surprise, very well, particularly on the dice loss. None of the models, however, proved that they learned the new expanding hyphae growth patterns, but most of them instead made uniform predictions around the skeleton.


\section{Discussion}

\subsection{Overfitting and Underfitting}
\subsubsection{Overfitting}
A typical indicator of overfitting is that the test and validation losses are significantly higher than the training loss. During the model hyperparameter experiments, we did not observe any alarming difference between the training and validation loss. Throughout the training, there are cases of deviation, but they follow each other closely.
For almost all the models, the validation loss minimum was very close to the training loss; the parameters associated with the minimum validation loss are the ones that are extracted and used for modelling. This implies that the parameters used are not overfitted to the training data.
Small datasets can lead to overfitting as well. It is known that many deep learning models are trained on datasets consisting of millions of samples. This is, however, not the norm in medical and biological modelling, since data is expensive to produce; here, a dataset with $5500$ samples is considered large. Only $70\%$ of the data is reserved for training, further, some of those samples are removed because they are too short.
Some would also argue that having three layers and 64 hidden channels is too complex a model for simple skeletonized biological data, which could also lead to overfitting. However, there is no sign of overfitting in our evaluations of our models.
\subsubsection{Underfitting}
There were clear signs of underfitting in some of the models we ran, this was in the $8$ hidden channels model and the $50$ future frames model. The learning curve was close to constant through the entire training, the validation and training loss were close to each other, and they failed at capturing the underlying structure in the data.
More generally speaking, on all the models, there were problems with the loss not changing that much from beginning to end. This might have been because of the vanishing gradient problem. In the theory section on LSTM, the vanishing gradient problem was addressed, and LSTM generally fixed this problem. This is, however, not entirely the case; the vanishing gradient problem is still there, just to a lesser extent than the first RNNs.
When conducting the experiments on different foreground percentages, some of the deeper models, especially, showed signs of underfitting. This was likely because Focal BCE punishes confidently wrong guesses harshly, so because of the high amount of foreground, it was tough to guess correctly, and the loss is minimized by not being confident and uniforming its guess.
\subsection{Deep vs wide networks}
Depth refers to the number of layers in the network, and width refers to the number of hidden channels. The benefits of deeper models are that they can model temporal data better, and they enable compositional representations, meaning it can dedicate each layer can be dedicated to a feature, for example, edges.
Deeper networks also take longer to train; they suffer from vanishing gradients, especially true for RNNs, and adding layers increases memory usage significantly. We observed in our trials that the deeper model had a lower validation and test loss. This could be a result of the decrease in blur due to the temporal efficiency. They also took longer to train.
The benefits of making a network wider are that they are easier to train than deeper models, and it can learn more complex spatial detail. That said, it increases parameter count, and too much width can make the model overfit.
Generally, we think the models could have benefited from being wider; it was a consistent problem that the models couldn't predict the spatial complexities of the expanding mycelium network and hyphae.


\subsection{Interpretation of results}
Overall, the analysis suggests that deeper and wider models perform better than shallow and narrow models on the prediction of mycelium growth patterns. It was a repeating problem that the models did not capture the specific pattern of the mycelium network, but rather just that there would be growth. Now, so far, the loss statistics have shown that the wider and the deeper the models are, the lower the validation, test, and training loss go, and there are no signs of overfitting. I think it is reasonable to say the ConvLSTM is capable of capturing these features, however, it would take powerful hardware to train such a model.
An observation that was made across the board for the prediction was that over time, the prediction frames got more and more blurry, which is known as probabilistic blur. It could symbolize that the mycelium does not grow predictably, but rather it does randomly; this is nonetheless not likely. We propose it's because there is an accumulation of error through time since the model is autoregressive, and because the BCE punishes wrong guesses too harshly. The failure to accurately predict the hyphae growth could also be attributed to the rather varied and medium-sized dataset.
Even though all the mycelium is that of the \textit{Saccharomyces cerevisiae}, there are still different strains that might have different growth patterns.
The experiments also indicate that lower foreground percentages are beneficial for the visual appearance of the predictions; however, worse for loss statistics.
The best model achieved for the $80$ frames prediction task on the sparse foreground dataset (dataset 3) is the $3$ layer deep and $64$ hidden channels wide ConvLSTM model. It had an average test loss of $0.2426$, which is considered a low loss function using BCE, which is used with a $0.6$ weight in our comboloss. Since BCE punishes confident wrong guesses harder, the background will contain some probabilities above $0$, and this will count towards the loss, but can easily be sorted using a threshold. An average dice loss of $0.2195$ is also considered good for segmentation tasks. For binary values like ours, it means that after the $0.5>$ threshold, $78.05\%$ of the predicted pixels and the target pixels overlap.
Now, if we put this in the context of $80$ predicted frames, the model performed very impressively. It's not an easy task to predict the future, and to do so spatially is even more challenging. There is error accumulation and just an increased amount of uncertainty the further the prediction horizon is, and $80$-frame forecast is considered long-term for ConvLSTM.
This becomes clear if we look at the performance of the model with the same model architecture but trained to predict $20$ frames instead. The loss statistics are $\text{average test loss} = 0.1913$ and $\text{Average Dice Loss} = 0.1621$, this is not considered outstanding for $20$-frame forecasting, but is still in the good to very good performance range.
\subsection{Strength and limitations}
The use of cross-validation is crucial for evaluating unbiased and useful models. The fact that the dataset is split up into three segments and we therefore achieve unbiased evaluation, makes it so the conclusions and statistics we reach are more likely to be the true reflection of the model's performance.
ConvLSTM is a strong model; it severely lessens the impact of the vanishing gradient problem, and it utilizes the powerful convolution operator, which has proven itself as one of the most efficient ways for learning spatial features. The data optimization pipeline has proven itself efficient at binarizing and skeletonizing the images in a way that excludes noise. Further, the loss function has been chosen in such a way that it is efficient at learning sparse foreground skeleton networks, where detail is important.
Overall, the loss statistics indicate that the models are performing well and manage to capture a decent amount of features. It shows that it is possible to get good results predicting lengthy horizons. The qualitative results were also promising and showed some recognition of patterns, despite not predicting individual hyphae.
The findings of this thesis suggest that spatiotemporal biological systems might not be as simple as many assume they are. Results and metrics got better the deeper and wider the model went. There might be greater advantages to going even deeper and wider than we went in this thesis, which is an exciting finding.
Being able to accurately model the growth of mycelium over time is extremely valuable. The main components of vitamins, amino acids, enzymes for textile production, detergent, and medicine are based on fermentation technology, and yeasts are a crucial part of that. Further in the introduction, we discussed the link between optimization and mycelium networks, If one manages to accurately predict mycelium growth, then one might be able to use the same model for optimizing networks.


A good dataset is essential for the success of training machine learning models. The dataset we used in this thesis is rather robust, but there are some challenges. Mycelium can easily get contaminated or go into hibernation, which prevents the mycelium from growing and showing any real spatial change over a period of time. Some of the samples in this dataset do not seem to be growing or change very slowly over time. This is a problem because it makes it so the model has to learn this dimension of growing and non-growing mycelium, which can be tough since they act so differently. This could also be a strength, because the use case for the model increases if it can successfully predict which of the two events will happen.
The optimization pipeline is not perfect, there is an element of randomness to what hyphae get shown in the final processed image. It might be because of lighting changes or other stuff that makes it so the pipeline doesn't always show the same skeleton network. However, it seems like the model averaged out the hyphae, so that faint hyphae that didn't always show up after the processing would still be predicted, just not with the same amount of certainty.
The predictive blur happening over time is creating noise and removing the contrast and complexity of the network. This is a common problem with ConvLSTMs and is certainly a limitation to what is achievable over long prediction horizons. When training the models, the training data is shuffled when it is loaded, which ensures variance in the ordering, so the model doesn't learn the order of the training data instead of the underlying distribution. This, however, also adds an element of randomness to training models; sometimes results can't be reproduced, or some models will get stuck in the wrong local minima. This is not ideal because it blurs the line between what is random and what is deliberate, and you can't just rule a model out the first time it is trained.
The main limitation of Deep Learning models is the computational power it takes to train them. In this project, we used a state-of-the-art GPU for training, and throughout the project saw limitations in memory and computational power. There was a total of 477 GPU hours used to train the models in this project. The required hardware is expensive and hard to come by, the hardware is highly energy-consuming, and the time it takes to train the models puts limitations on accessibility and research.
\subsection{Improvements}
Ordinary differential equations, transformers, do some strain identification, and predict contamination
We would include an experiment where we tested adding the strain as an input, this could make it so the model could have different parameters for different strains, in case they grow differently. One could also add two output dimensions by using a classification head, one to determine the strain of the input and one to determine if the yeast is contaminated or will see significant growth.
Further, a mask for the temporal padding could improve the learning of long-term dependencies. One could also make the ConvLSTM bidirectional, meaning it can consider past and future context. It could also be interesting to compare autoregressive models to non-autoregressive models, the tests suggest that there is quite a bit of error accumulation, maybe using non-autoregressive architecture could prevent this.
Transformers are integral to the deep learning revolution we have seen in Natural Language Processing. They are highly versatile and have shown great promise in learning long-term dependencies because they don't have the vanishing gradient problem. There are a lot of different transformer architectures for learning spatiotemporal dependencies, and we think it could be exciting to test these against the ConvLSTM.


\section{Conclusion}
This thesis examined different architectures of the ConvLSTM model for forecasting skeletonized and binarized mycelium growth with a long prediction horizon. The autoregressive models with 1, 2, and 3 layers, $32$ hidden channels, $1$ greyscale input channel, $80$-frame forecast, $3 \times 3$ kernel, $1$ stride, $2$ wide zero padding, no bias and no peephole connections, was trained on datasets with varying foreground percentages. The foreground percentage was controlled through dilation, and the foreground that exhibited the best behavior was dataset $3$ with $24.12\%$ foreground. This was concluded mainly from the qualitative results.
Further, we compared models with varying widths and depths trained on dataset $3$. Overall, most of the models showed good loss statistics compared to the rather long forecast horizon, but the overall theme was that wider and deeper models did better than shallow and narrow models.
Our best model, with $3$ layers, $64$ hidden channels, and $80$-frame forecast, saw an average test loss of $0.2426$ and an average dice loss of $0.2195$, this is considered very good for its task. All the produced visual qualitative data produced by the models struggled with what is likely the accumulation of error as the prediction sequence progresses in time. This is normal behavior for ConvLSTM, nonetheless, it was problematic and made it so the model couldn't accurately predict the complexities of the individual hyphae growth and the expansion of the mycelium net.
It could be interesting to test a deeper and wider model on the data and see if there could be further improvements to the predictions, maybe even add an input dimension for strain type. Transformers have shown incredible performance and versatility. It could be interesting to compare the performance of the ConvLSTM to that of a transformer.
Fermentation technology is of critical importance in the production of many products; modelling of yeast growth patterns could improve the fermentation process further.




\appendix

\addcontentsline{toc}{section}{Appendix}

\hypertarget{sec:loss}{\subsection{Loss functions}}

\begin{lstlisting}
  #Loss functions
  def dice_loss(pred, target, mask=None, smooth=1e-3):
      pred = pred.contiguous()
      target = target.contiguous()
      if mask is None:
          mask = torch.ones_like(target)
  
      intersection = (pred * target * mask).sum(dim=(2, 3))
      numerator = 2. * intersection + smooth
      denominator = (pred * mask).sum(dim=(2, 3)) + (target * mask).sum(dim=(2, 3)) + smooth
      loss = 1 - numerator / denominator
      return loss.mean()
  
  def focal_bce_loss(logits, target, gamma=2.0, pos_weight=None, mask=None):
      prob = torch.sigmoid(logits)
      bce = F.binary_cross_entropy_with_logits(logits, target, reduction='none', pos_weight=pos_weight)
      pt = prob * target + (1 - prob) * (1 - target)
      focal_weight = (1 - pt) ** gamma
      loss = focal_weight * bce
      if mask is not None:
          loss = loss * mask
      return loss.sum() / (mask.sum() + 1e-8)
  
  
  
  def dice_score(pred, target, mask=None, smooth=1e-3):
      pred_bin = (torch.sigmoid(pred) > 0.5).float()
      target = target.float()
      if mask is None:
          mask = torch.ones_like(target)
  
      intersection = (pred_bin * target * mask).sum(dim=(2, 3))
      numerator = 2. * intersection + smooth
      denominator = (pred_bin * mask).sum(dim=(2, 3)) + (target * mask).sum(dim=(2, 3)) + smooth
      score = numerator / denominator
      return score.mean()
  
  
  
  def full_combo_loss(logits, target, mask=None, gamma=2.0, lambda_focal=0.6, lambda_dice=0.4):
      if mask is None:
          mask = torch.ones_like(target)
  
      with torch.no_grad():
          num_ones = (target * mask).sum()
          num_zeros = mask.sum() - num_ones
          ratio = num_zeros / (num_ones + 1e-8)
          safe_ratio = torch.clamp(ratio, max=100.0)
  
      pos_weight = torch.tensor([safe_ratio.item()], device=logits.device)
  
      focal = focal_bce_loss(logits, target, gamma=gamma, pos_weight=pos_weight, mask=mask)
  
      dice = dice_loss(torch.sigmoid(logits), target, mask)
  
      return lambda_focal * focal  + lambda_dice * dice
\end{lstlisting}
\hypertarget{sec:training}{\subsection{Training loop}}

\begin{lstlisting}
  for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")

    for x_input_padded, y_target, mask in pbar:
        mask = mask.to(device, non_blocking=True)
        x_input_padded = x_input_padded.to(device, non_blocking=True)
        y_target = y_target.to(device, non_blocking=True)

        optimizer.zero_grad()

        with autocast():
            y_pred = model(x_input_padded, T_future)
            loss = full_combo_loss(y_pred, y_target, mask=mask, **loss_params)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        loss_val = loss.item()
        running_loss += loss_val
        pbar.set_postfix({'Batch Loss': loss_val})

        checkpoint_train_logs.append({
            'step': global_step,
            'epoch': epoch + 1,
            'avg_train_loss': loss_val,
        })

        global_step += 1

        del x_input_padded, y_target, y_pred, loss

    torch.cuda.empty_cache()

    avg_train_loss = running_loss / len(train_loader)
    print(f"[Epoch {epoch+1}] Avg Train Loss: {avg_train_loss:.4f}")

    model.eval()
    val_loss = 0.0
    dice_scores = []
    with torch.no_grad():
        for x_val, y_val, mask_val in val_loader:
            x_val = x_val.to(device, non_blocking=True)
            y_val = y_val.to(device, non_blocking=True)
            mask_val = mask_val.to(device, non_blocking=True)

            with autocast():
                y_val_pred = model(x_val, T_future)
                val_l = full_combo_loss(y_val_pred, y_val, mask=mask_val, **loss_params)
                val_loss += val_l.item()

                score = dice_score(y_val_pred, y_val, mask_val)
                dice_scores.append(score.item())

    avg_val_loss = val_loss / len(val_loader)
    avg_val_score = sum(dice_scores) / len(dice_scores)
    print(f"[Epoch {epoch+1}] Val Loss: {avg_val_loss:.4f}, Dice Score: {avg_val_score:.4f}")

    checkpoint_logs.append({
    'step': global_step,
    'epoch': epoch + 1,
    'avg_train_loss': avg_train_loss,
    'avg_val_loss': avg_val_loss,
    'val_dice_score': avg_val_score,
    })


    #Save best model
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        print(f"New best model found! Saving checkpoint with val loss = {avg_val_loss:.4f}")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': avg_val_loss
        }, checkpoint_path)

\end{lstlisting}
\hypertarget{sec:optim}{\subsection{Data skeletonization pipeline}}

\label{app:optim}
\begin{lstlisting}
  def skeletonize_tensor(self, image):
  img_gray = image.convert('L')
  tensor = transforms.ToTensor()(img_gray)  #(1, H, W)

  #Skeletonization
  frame_np = tensor.squeeze(0).numpy()  #(H, W)

  frame_cleaned = np.where(frame_np > 0.2, frame_np, 0)
  background = cv2.GaussianBlur(frame_cleaned, (51, 51), 0)
  flattened = frame_cleaned / (background + 1e-8)
  flattened = np.clip(flattened, 0, 1)

  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
  flattened_8bit = (flattened * 255).astype(np.uint8)
  flattened_clahe = clahe.apply(flattened_8bit)

  binary_adaptive = cv2.adaptiveThreshold(
      flattened_clahe,
      1,
      cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
      cv2.THRESH_BINARY,
      blockSize=35,
      C=0
  )

  binary_closed = closing(binary_adaptive, footprint_rectangle((3, 3)))
  skel = skeletonize(binary_closed)

  skel_tensor = torch.from_numpy(skel).unsqueeze(0).float()

  
  skel_np = skel_tensor.squeeze().numpy()
  skel_binary = skel_np > 0.1

  skel_dilated = dilation(skel_binary, disk(0))

  skel_pil = Image.fromarray((skel_dilated * 255).astype(np.uint8))
  skel_processed = self.transform(skel_pil)

  skel_tensor = (skel_processed > 0.1).float()


  return skel_tensor
\end{lstlisting}


\addcontentsline{toc}{section}{References}

\bibliographystyle{plain}
\bibliography{References}  % If you have some references, use BibTeX

\end{document}