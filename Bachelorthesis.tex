\documentclass[a4paper,12pt]{article}

\usepackage{a4wide}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lipsum}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}  % For including images
\usepackage{subfigure}  % In preamble
\usepackage{bm}
\usepackage{xcolor}  % For a colorfull presentation
\usepackage{listings}  % For presenting code 

\usepackage{hyperref}

% Definition of a style for code, matter of taste
\lstdefinestyle{mystyle}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  backgroundcolor=\color[HTML]{F7F7F7},
  rulecolor=\color[HTML]{EEEEEE},
  identifierstyle=\color[HTML]{24292E},
  emphstyle=\color[HTML]{005CC5},
  keywordstyle=\color[HTML]{D73A49},
  commentstyle=\color[HTML]{6A737D},
  stringstyle=\color[HTML]{032F62},
  emph={@property,self,range,True,False},
  morekeywords={super,with,as,lambda, for, do, end, return, if, then},
  literate=%
    {+}{{{\color[HTML]{D73A49}+}}}1
    {-}{{{\color[HTML]{D73A49}-}}}1
    {*}{{{\color[HTML]{D73A49}*}}}1
    {/}{{{\color[HTML]{D73A49}/}}}1
    {=}{{{\color[HTML]{D73A49}=}}}1
    {/=}{{{\color[HTML]{D73A49}=}}}1,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=none,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=4,
  frame=single,
}
\lstset{style=mystyle}

\begin{document}
\title{Bachelor Projekt proposition (2025)\\ Eksempel}
\author{\color{red}Hans-Christian MÃ¸ller Zerrahn - dhw960@alumni.ku.dk}
\date{}
\maketitle

% Please leave the table of contents as is, for the ease of navigation for TAs
\tableofcontents % Generates the table of contents
\newpage % Start a new page after the table of contents
\section{Abstract}
This thesis explores the ConvLSTMs ability to predict the growth pattern of the mycelium associated with the yeast \textit{Saccharomyces cerevisiae} over time. The sequences of growth has been binarized and then skeletonized so only the mycelium network remains, this is the data the models are trained on and the form of the predictions as well.
It's an autoregressive model making up to $80$-frames long predictions, the models are varying in depth, width, prediction horizon and the foreground percentage of the datasets. The reasoning behind the variation is to test the models performance under different conditions.
We found that deeper and wider models trained on a sparse foreground dataset had the best performance when evaluating on loss statistics and qualitative visual data. The best $80$-frame prediction model had $64$ hidden channels and $3$ layers, it had an average dice score of $0.7805$ and visually seems to capture the spatial features very good and the temporal features decently.
Generelly the models all seemed to suffer from probalistic-blur in the later part of the prediction sequence. This made the models unable to learn and predict the intrinsic thin hyphae growth in the mycelium network. The findings seems to suggest that mycelium networks growth over time might be more complex than expected and that more complex models achieve better results.
Overall the modelling of complex mycelium networks seems feasible using ConvLSTM, directions for future work could include deeper and wider models as well as implementation of transformer architecture.
\section{Problem formulation}
Deep learnings neural networks has proven to be an effective method for spatiotemporal forecasting. Being able to make accurate predictions of future events has proven valuable, it prevents mistakes and improves decision quality. In this thesis, we will be applying the methods of deep learning to make predictions on mycelium growth patterns.
To be more precise, based on sequential images of the initial growth of mycelium, the model will output a prediction sequence of images of future growth. Being able to predict future growth of mycelium has many use cases. Firstly one would be able to predict whether or not the mycelium will keep growing, this helps determine if the sample should keep growing or be scratched. Secondly some mycelium has shown to grow in ways that are optimal for fx. subway networks, this is explored in \cite{joseph2022slime}. 
Which further adds incentives to learn and study the growth of mycelium. For this objective we will be using Convolutional Long Short-Term Memory (ConvLSTM) networks, because it combines the capturing of temporal (LSTM) and spatial (Convolution) dependencies, which is something most time-series models are not capable of. There is a generel problem with Deep Learning models getting stuck in local minimums during training. Moreover when making long prediction sequences, there usually is a lot of variance and predictions get unprecise. At last the number one problem with spatiotemporal models is their heavy memory usage.
The topic of this thesis is to study how effective the ConvLSTM is at making predictions of variable sequence length. What effect the different hyperparameters, as well as different loss functions, has on the models soundness.
\section{Theoretical background}
\subsection{Neural Network}
A neural network is a machine learning model type that mimicks the structure and function of biological neural networks. Much like the brain of a living being, it is composed of neurons and synapses. The neurons communicate through the signal in synapses, neurons take these signals as inputs passes them through an activation function for output.
The neural network has an input layer, one or more hidden layers and an output layer. Each layer is built of neurons stacked on top of each other and has the same way of handling input.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{images/ANN.png}
  \caption{Representation of a Neural Network. there is the three different types of layers, we see the neurons symbolized by nodes and synpases symbolized by edges. (reproduced from \cite{glosser_ann})} % Dummy caption generated using lipsum
  \label{fig:n1}
\end{figure}
Every layer except the input layer has an activation function. An activation function is typically a non-linear function, this is because if a neural network only has linear activation functions the entire network is equivalent to a single layer model.
Typical examples of activation functions include: ReLU, Sigmoid / logistic, hyperbolic tangens, exponential LU. The edges are usually some weight which is used to calculate the signal represented by a linear combination on the input.

Multi-layered networks is one of the simplest forms of neural networks and often serves as the standard example. The following example is based on \cite{bishop2024deep}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{images/NN.png}
  \caption{Representation of a multi-layered Neural Network.(reproduced from \cite{bishop2024deep})} % Dummy caption generated using lipsum
  \label{fig:n2}
\end{figure}
The weights of the edges into the neuron define the input into the associated activation function. The superscript $(1)$ is the symbol for being on the first layer and $j=1,\dots, M$.
\begin{align}
  a^{(1)}_j = \sum_{i=1}^{D} w^{(1)}_{ji} x_i + w^{(1)}_{j0}
\end{align}
The activation then takes that input and outputs the value of that node.
\begin{align}
  z_j^{(1)} = h\left(a_j^{(1)}\right)
\end{align}
Now the value of that node is used for the calculation of the input to the next activation function.
\begin{align}
  a_k^{(2)} = \sum_{j=1}^{M} w_{kj}^{(2)} z_j^{(1)} + w_{k0}^{(2)}
\end{align}
For $l=2,\dots,L-1$ and L being the total number of layers, we can start the recursion of $a_j^{(l+1)}$ and $z_j^{(l)} $
\begin{align}
z_j^{(l)} = h_l\left(a_j^{(l)}\right) \\
a_j^{(l+1)} = \sum_i^M w_{ji}^{(l+1)} z_i^{(l)} = \sum_i^M w_{ji}^{(l+1)} h_l\left(a_i^{(l)}\right)
\end{align}
Then it is easy to find the output
\begin{align}
  y_j = h_L\left(a_j^{(L)}\right)
\end{align}
It is crucial to keep in mind that neural network (NN) is an umbrella term and therefore the term can be somewhat loose and two neural networks can be vastly different.
\subsection{Recurrent neural networks}
Recurrent neural networks (RNN) are a subcategory of neural networks, designed for sequential data, so that previous hidden states used to find outputs can be used as inputs for the next timestep.
This makes it so the model can retain important information from the previous time step and use that information in the calculations of the output of current time step. 
The following is based upon \cite{amidi_rnn_cheatsheet}, this provides an introduction to RNNs.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{images/architecture-rnn-ltr.png}
  \caption{Representation of the architecture for a typical recurrent neural network (RNN), (reproduced from \cite{amidi_rnn_cheatsheet})} % Dummy caption generated using lipsum
  \label{fig:n3}
\end{figure}
Now this should remind you of a Hidden Markov Model (HMM) because of the sequential structure and temporal dependence, one can think of RNNs as the differentiable version of an HMM.

Typically the activation / hidden state $a^{\langle t \rangle}$ will look like a variation of this, for some activation function $\sigma_1$:
\begin{align}
  a^{\langle t \rangle} = \sigma_1\left(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a\right)
\end{align}
The hidden state is used as part of the input to the activation function $\sigma_2$ for the calculation of the output $y^{\langle t \rangle}$
\begin{align}
  y^{\langle t \rangle} = \sigma_2\left(W_{ya} a^{\langle t \rangle} + b_y\right)
\end{align}
Where the weights $W_{aa}, W_{ax}, W_{ya}$ and biases $b_a, b_y$ are shared temporally.

What typically changes in different RNN's is the way the reccurent update happens. Changes usually occur because you want to make it specific for your task or to combat some problems you're having. One of the most common problems is the Vanishing / exploding gradient phenomena.
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.8\linewidth]{images/description-block-rnn-ltr.png}
%   \caption{Representation of a vanilla RNN recurrent update block} % Dummy caption generated using lipsum
%   \label{fig:n4}
% \end{figure}
\subsubsection{Exploding / vanishing gradient problem}
In standard RNN there is a problem with finding gradients for the loss function w.r.t. the weights because as we showed in the backpropagation section we find the gradient as follows:
\begin{align}
  \frac{\partial L}{\partial h_k} = \frac{\partial L}{\partial h_t} \cdot \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}}
\end{align}
This is a problem for very high values of t because if we look at this term:
\begin{align}
  \frac{\partial h_i}{\partial h_{i-1}}
\end{align}
In this matrix for the eigenvalues that are less than one they will vanish and the ones that are greater than one will explode. This is because the term is multiplied together in a long consecutive chain.
We observe that for $\frac{\partial h_i}{\partial h_{i-1}}$ with all eigenvalues less than one:
\begin{align}
  \left\| \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} \right\| \to 0 \quad \text{as } t - k \to \infty
\end{align}
Conversely for eigenvalues greater than one we observe:
\begin{align}
  \left\| \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} \right\| \to \infty \quad \text{as } t - k \to \infty
\end{align}
This is known as the exploding / vanishing gradient problem typical for RNNs.
\subsection{Long short-term memory}
The long short-term memory (LSTM) network, which is a gated RNN, was made to fix the problem RNNs had with exploding / vanishing gradients. 
Instead of reccurently connecting hidden units like in a normal RNN, LSTM connects cell blocks made up of non-linear and linear operations on different gates.
A LSTM cell consists of an input, three gates, a cell update, cell state and a hidden state (the output). This section is based on \cite{goodfellow2016lstm}.

A gate is a learned mechanism for controlling the amount of information that flows through a cell.
There is three different types of gates in a cell in a LSTM, the forget gate decides how much of old information to keep, the input gate decides how much of new information to introduce into memory and the output gate decides how much of the memory we want to output.
\begin{align}
  f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
  i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
  o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) 
\end{align}
Where $f_t$ is the forget gate, $i_t$ is the input gate and $o_t$ is the output gate and $\sigma$ is the sigmoid function.

The cell update is the mechanism deciding how we should update our cell state. It has much like the gates, the previous hidden state and the current input as input variables. It decides what to write in the new cell state based on the new information from the previous cell.
\begin{align}
  \tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
\end{align}

The cell state can be intuitively seen as the long-term memory of the network. Each cell in the recurrent network has a state that symbolizes the memory at that time. The information flows chronologically up through the cells and at each cell it calculates how much information of the previous cell state should be kept and how much new information to put into memory. 
The cell state is defined by the forget gate, the previous cell state, the input gate and the cell update. The forget gate determines how much of the previous cell state to keep, this is usually pretty close to one. The input gate determines how much of the cell update should be added to the cell state.
Due to the current cell states dependence upon the previous it makes it so that all cell states at time  $t_n$ are dependent upon all previous cell states at time $t_i<t_n$, unless $f_t=0$  then all memory is wiped and the new chain of dependence starts at $t$.
\begin{align}
  c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
\end{align}
Where the symbol $\odot$ represents the element-wise product / hadamard product.

The last part of the cell calculates the hidden state. It is the element-wise product of the output gate and $\tanh(c_t)$. The output gate controls how much of the cell state we want to output. The hidden state is the output of the cell.
\begin{align}
  h_t = o_t \odot \tanh(c_t)
\end{align}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{images/LSTM.png}
  \caption{Cell architecture in a LSTM. (reproduced from \cite{goodfellow2016figure})} % Dummy caption generated using lipsum
  \label{fig:n5}
\end{figure}
LSTM solves the vanishing / exploding gradient problem by using self-loop on the cell-states. Because it uses the previous cell-state to calculate the current cell-state, it creates a linear path for the gradient through time. This is easy to see when we remember how the gradients are calculated through back-propagation or when we explored the vanishing / exploding gradient problem for generel RNN.
\begin{align}
  \frac{\partial L}{\partial c_t} = \frac{\partial L}{\partial c_T} \prod_{i=t+1}^{T} \frac{\partial c_t}{\partial c_{t-1}}
\end{align}
But this is no problem because
\begin{align}
  \frac{\partial c_t}{\partial c_{t-1}} = f_t
\end{align}
Previously whenever the gradient hit zero it would be stuck there, because this is a minimum and therefore optimized, even though it wasnt optimzed. Now what is great about this is, that the gradient is controlled by $f_t$ so if one of them is zero, that means things are forgotten and we are in a minimum for the loss function w.r.t. the cell state. 
However because all of the gates are learnable, if the model now learns that it has to retain memory then it can change the forget gates and the gradient will no longer be stuck at zero. This is why we usually see $f_t \approx 1$ because the model learns that it is good to retain memory of previous states.
Of course the exploding gradient is no longer a problem since $0\leq f_t \leq 1$. Since all the weights and biases of the gates and cell update doesn't flow through time, the exponential growth or descent doesn't happen the same way.
\subsection{Convolutional neural networks}
Convolutional neural networks (CNNs) has had huge pratical succes. A CNN is a neural network that use convolution instead of matrix multiplication in minimum one of it's layers. The material presented in this chapter is adapted from \cite{goodfellow2016CNN}.
\subsubsection{The convolution operator}
The convolution operator is defined as:
\begin{align}
  s(t)=(x * w)(t)=\int x(a) w(t-a) d a
\end{align}
Where $x$ is typically our input / data we want changed and $w$ is a weighting function used to change the input / data.

We have the discrete convolution:
\begin{align}
  s(t)=(x * w)(t)=\sum_{a=-\infty}^{\infty} x(a) w(t-a)
\end{align}
This is for when $x$ and $w$ only has integers in their domain.

If we apply the discrete convolution to an image, the image is a two-dimensional matrix and we then use a two-dimensional kernel as weight, we get to following:
\begin{align}
  S(i, j)=(I * K)(i, j)=\sum \sum I(m, n) K(i-m, j-n)
\end{align}
Since convolution is commutative, due to the flipping of the kernel relative to the input, we can rewrite as:
\begin{align}
  S(i, j)=(K * I)(i, j)=\sum \sum I(i-m, j-n) K(m, n)
\end{align}
This is called 2-D convolution.

However practically convolution is almost never used. It is mostly used because of its utilty in proofs because of it's commutative property.
Most neural network libraries implement the cross-correlation, which doesnt have the commutative property, but is much easier to work with in practice.
\begin{align}
  S(i, j)=(K * I)(i, j)=\sum \sum I(i+m, j+n) K(m, n)
\end{align}
Cross-correlation will often be reffered to as convolution, because it practically does the same thing.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{images/2dconv.png}
  \caption{A generel example for 2-D convolution. (reproduced from \cite{goodfellow2016figure2})} % Dummy caption generated using lipsum
  \label{fig:n6}
\end{figure}
\subsubsection{Benefits of convolution}
There is three main benefits to using convolution with a kernel. 
\begin{enumerate}
  \item \textbf{Sparse interactions:} A typical fully connected neural network (as described in the neural network chapter) contains an edge between each node in layer $l-1$ and each node in layer $l$. 
  Each edge corresponds to a unique parameter, resulting in a dense connectivity pattern and a large number of parameters. In contrast, convolutional neural networks (CNNs) use a small kernel with size $W_{\text{kernel}} \times H_{\text{kernel}}$, which is applied across the input. 
  This reduces the number of parameters and results in sparse interactions between input and output, since each output unit depends only on a small part of the input. fx. CNN with kernel $3 \times 3$ and image $64 \times 64$, $\text{stride} = 1$, $\text{padding} = \text{"same"}$ then the number of connections is $\text{\#connections} = 64^23^2 = 36864$. Now look at a fully connected neural network with input image of dim $64 \times 64$, that has  $512$ number of hidden units, then the number of connections is $\text{\#connections} = 64^2\cdot 512=2097152$
  This means that computing the output requires fewer operations.
  \item \textbf{Parameter sharing: } In a typical fully connected neural network all parameters are uniquely used once when computing the output of a layer. However in a CNN parameters are shared, meaning that weights can be used mulitple times on different input. It follows the same growth as above in terms of computation time. So by sharing parameters the number of computations and memory that has to be used is significantly lowered. Because of the lower number of parameters, the model needs less data to train on, because there is less parameters to train. Further we see that there is less overfitting when using fewer parameters.
  \item \textbf{Equivariant representations: } Equivariance between to functions $g(x)$ and $f(x)$ means that $g(f(x)) = f(g(x))$. Intuitively this just means no matter in which order the functions are applied the results will be equivalent. For convolution the type of parameters sharing makes it so the layer is equivariant to translation. Meaning translation that shifts the input, lets say $g(I)(i,j)=I(i - 1,j - 1)$ then $\left(K * g(I)\right)(i, j)=(K * I)\left(i-1, j-1\right)$.
\end{enumerate}
\subsubsection{Pooling}
The pooling operation on two dimensional matrices pools together a region and outputs a singular value. Examples of pooling include max pooling and average pooling. Max pooling outputs the max value of the values included in its region (pools the whole region together to the maximum value). The average pool finds the average of the region and outputs it.
Pooling helps with invariance, because it pools together a region and unifies it into one value. This leads to the fact that small variances will be smoothed out by the pooling.
When you add pooling it's like assuming an infinitely strong prior. The prior being that the learned function is invariant to minor translations. This will heavily benefit the statistical efficency, granted the prior assumtion is correct.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{images/maxpool.png}
  \caption{Example of max pooling. (reproduced from \cite{maxpool_cs_wiki})} % Dummy caption generated using lipsum
  \label{fig:n7}
\end{figure}
\subsubsection{Structure of CNN}
All in all a CNN consists as a minimum of five layers: input, convolution, activation function, pooling and an output layer. 
The input and output layers are selfexplanatory necessary, the input layer is made up of number of dimensions and number of channels. In the case of an image there would be two dimensions (height and width) and three channels for RGB images and one channel for greyscale images. The output layer can really vary quite a bit depending on the prediction, it could be an image or a number of classes associated with a probability.
The convolution layer is where the model has its parameters / kernel and is therefore where the model learns. It detects local patterns and outputs these as hidden channels. In the convolution layer the number of parameters depend upon number of input channels and hidden channels aswell as the size of the kernel. Imagine Input: $(C_{in}, H, W)$. Output: $(C_{out}, H_{out}, W_{out})$ for the convolution then there is a kernel for each pair of input channel and output channel. These output channels / hidden channels are feature maps, they have the capability to represent for the same input a different feature.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{images/CNN.png}
  \caption{Example of a CNN (reproduced from \cite{bishop2024deeplearning})} % Dummy caption generated using lipsum
  \label{fig:n8}
\end{figure}
Earlier we discussed why there has to be a non-linear activation function in between convolutional layers, but it is basically to preserve the complexity of the model and not just let it be boiled down to one linear model.
The pooling layer has been discussed in the above chapter, it makes the network more invariant and lessens the number of computations. Sometimes there is a fully connected layer, this layer sums up all the learned features and helps point at a conclusion.
\subsubsection{2D Cross-correlation}
The following is based on the library Pytorch's class \cite{pytorch_conv2d}.

Input: $(N, C_{in}, H, W)$. Output: $(N, C_{out}, H_{out}, W_{out})$.
\begin{align}
    out_{i, j} = b_{j}+\sum^{C_{in}-1}_{k=0} \text{Kernel}_{j, k} \star input_{i, k}
\end{align}
Where N is number of batches, $C_{in}$ is number of input channels, $C_{out}$ is number of output channels, H is the height of the input, W is the width of the input, $H_{out}$ is the height of the output, $W_{out}$ is the width of the output, i is the index for N and j is the index for $C_{out}$ and $\star$ is the cross correlation operator. i is the index for the batch and j is the index for the output channel. For each possible combination pair of batch and output channel the $out_{i, j}$ is calculated.\\ 
The dimensions of the output is calculated by:
\begin{align}
    H_{out}&=\left\lfloor\frac{H_{in}+2\times\mathrm{padding}[0]-\mathrm{dilation}[0]\times(\text{kernel size}[0]-1)-1}{\mathrm{stride}[0]}+1\right\rfloor\\
    W_{out}&=\left\lfloor\frac{W_{in}+2\times\mathrm{padding}[1]-\mathrm{dilation}[1]\times(\text{kernel size}[1]-1)-1}{\mathrm{stride}[1]}+1\right\rfloor
\end{align}
\subsection{Convolutional LSTM}
By combining the capabilities of the LSTM and the CNN we get the convolutional LSTM. The main structure is still the LSTM, because we are using the same Cell architecture. We are just using different operands and different dimensions. By having the inputs $x_1, \dots, x_t$, cell states $c_1,\dots, c_t$, hidden states $h_1, \dots, h_t$ and the gates $i_t, f_t, o_t$ all be 3D tensors with dimensions (Channels, width, height), we can now input spatial data into our model.
The notation and setup closely follows \cite{Shi2015ConvolutionalLN}.
To handle these new dimensions we are using the convolution operator $(*)$. All the weights
\begin{align*}
  W_xi, W_xf, W_hc, W_xo \in \mathbb{R}^{C_{hidden} \times C_{input} \times kH \times kW}\\
 W_hi, W_hf, W_hc, W_ho \in \mathbb{R}^{C_{hidden} \times C_{hidden} \times kH \times kW} 
\end{align*}
 are kernels of 4 dimensions, mean while the weight 
 \begin{align*}
  W_ci, W_cf, W_co, \in \mathbb{R}^{C_{hidden} \times H \times W}
 \end{align*}
 is a parameter tensor and 
 \begin{align*}
   b \in \mathbb{R}^{C_{hidden} \times 1 \times 1}
 \end{align*}
is a bias vector.
Here we still use $\odot$ as the symbol for hadamard.
\begin{align}
i_t &= \sigma(W_{xi} * \mathcal{X}_t + W_{hi} * \mathcal{H}_{t-1} + W_{ci} \odot \mathcal{C}_{t-1} + b_i) \\
f_t &= \sigma(W_{xf} * \mathcal{X}_t + W_{hf} * \mathcal{H}_{t-1} + W_{cf} \odot \mathcal{C}_{t-1} + b_f) \\
\mathcal{C}_t &= f_t \odot \mathcal{C}_{t-1} + i_t \odot \tanh(W_{xc} * \mathcal{X}_t + W_{hc} * \mathcal{H}_{t-1} + b_c) \\
o_t &= \sigma(W_{xo} * \mathcal{X}_t + W_{ho} * \mathcal{H}_{t-1} + W_{co} \odot \mathcal{C}_t + b_o) \\
\mathcal{H}_t &= o_t \odot \tanh(\mathcal{C}_t)
\end{align}
As we can see the parameters are reused across all time frames, only the gates, hidden state and cell state are changing over time. In practice the parameters $W_ci, W_cf, W_co$ are really just vectors of hidden channels length, which are then broadcasted up so each scalar now is a $height \times width$ matrix. 
The gates are further changed by the peephole connections with the peephole weigths $W_ci, W_cf, W_co$, we have $W_{ci} \odot \mathcal{C}_{t-1}, W_{cf} \odot \mathcal{C}_{t-1}, W_{co} \odot \mathcal{C}_t $. We are including a term with the previous cell state, so the gates can change according to what is currently in memory. However the peephole implementation isn't a must do and one should be aware that it adds weights and therefore computation time.

\subsection{Loss function}
Loss function is the function used to measure error. There is plenty of loss functions to pick from depending on the scenario some are more appropiate than others.
Since the loss function is, what we are measuring our error on and are using to optimize our model, the choice of loss function matters a lot. The notation used for loss functions throughout this paper is $L(f(x; \theta), y)$ where $f$ is the model making a prediction. The generel goal is for $f(x; \theta) \approx y$, which is what we hope to achieve by minimizing the loss function.
One has to be aware of overfitting and keep in mind that most outcomes has a variance, therefore a bit of error is acceptable and the goal in reality is try and find a model mimicking the underlying / true model for the data.
\subsubsection{Dice loss}
The dice Score is a measure for how many elements two sets has in common
\begin{align}
  \text{Dice Score} =\frac{2|P \cap T|}{|P|+|T|}
\end{align}

The principle of the dice Score can be transformed into a loss function called the dice loss. It looks at the predicted matrix and the target matrix and compares the values of the same entrance.
\begin{align}
  L_{\text {dice }}=1-\frac{2 \sum_{m,n} t_{m,n} p_{m,n}}{\sum_{m,n}\left(t_{m,n}+p_{m,n}\right)}
\end{align}
The Dice score is equal to one when we have the exact same values and zero for when one of the values are zero.  
\subsubsection{Weights}
The focal weight is used to increase to loss for hard predictions and reduce the loss for easy predictions. Firstly the probability on the predictions is found
\begin{align}
  \hat{P} = \sigma(\hat{y})
\end{align}
We then use this to find the probability of the true class
\begin{align}
  P_t = y \hat{P} + (1-y)(1-\hat{P})
\end{align}
The focal weight is then
\begin{align}
  Focal-weight = (1-P_t)^\gamma
\end{align}
$P_t$ is close to one when the predicted probability of the true class is close to one. This makes it so this $(1-P_t)^\gamma$ is close to zero, which is what we want because if $P_t \approx 1$ then it must be an easy prediction. However if $P_t$ is close to zero then it must be a hard prediction to make so the focal weight is closer to one, which isn't punished as hard. Typically $\gamma > 1$ because we want to punish lower values of $P_t$ harder in the loss.


% The function of distance weight is implied in the name. It is used to punish predictions that are further away from the target on an image harder than those close to the target. Imagine a binary matrix and zeroes are background and ones are foreground, then a matrix is created with the euclidian distance calculated to the nearest foreground pixel for each entrance in the input matrix.

% We create a binary mask $M \in {0,1}^{H \times W}$, where $M_{i,j} = 1$ is foreground and $M_{i,j} = 0$ is background. We get the following
% \begin{align}
%   D(i, j) = \min_{i^\prime, j^\prime} \sqrt{(i-i^\prime)^2+(j-j^\prime)^2} \text{   Given that    } M_{i^\prime, j^\prime} = 1
% \end{align}
% This is the distance weight for entrance $i, j$

\subsubsection{Binary Cross Entropy with sigmoid layer}
The BCE loss is for binary ground truth labels and prediction labels $p \in [0,1]$.
\begin{align}
  \text{BCE}(\hat{y}, y) = - \left( y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right)
\end{align}
intuitively we can see that the model punishes wrong guesses really hard due to how the natural logarithm scale for values in range $[0;1]$.

Taking the sigmoid function of the prediction works extremely well because it maps prediction to values in $[0,1]$. This is advantageous because it is binary events and we want to know the probability of a pixel being one or zero. $\sigma(0)=0.5$ this is great for binary tasks, because we can then use zero as a threshold for classification. Further it is differentiable and smooth, which is important since we have to find the gradient of the loss function.
\begin{align}
  l_{n}&=\left[p y_{n} \cdot \log (\sigma\left(x_{n}\right))+\left(1-y_{n}\right) \cdot \log \left(1-\sigma\left(x_{n}\right)\right)\right]
\end{align}
We are further adding a weight to the loss, this is done to adjust for class imbalance. If there is a majority of one class it can be a problem because its then more safe to bet on it being that class, so the weight is making up for the class imbalance.
\begin{align}
  p &= \frac{N-\sum^{N}_{n=1} y_n }{\sum^{N}_{n=1} y_n}
\end{align}
\begin{align}
  \ell(x, y)&=\frac{1}{N} \sum^{N}_{n=1} l_{n}
\end{align}
In the end we average over the loss for each pixel and get the average error on the prediction.

This loss is good for prioritizing generating correct pixels, binary prediction tasks and reducing the problem with class imbalance.

\subsection{Backpropagation}
The backbone of the Backpropagation algorithm is the chain rule.

Chain rule: If $g$ is differentiable at $x$ and $f$ is differentiable at $g(x)$, then the composite function $h=f \circ g$ is differentiable at $x$ and $h^\prime(x)$ is given by:
\begin{align}
  h^\prime(x)=f^\prime(g(x))\cdot g^\prime(x)
\end{align}
Using Leibniz notation, which is the notation moving forward, we have:
\begin{align}
  \frac{d f}{d x} = \frac{d f}{d g } \frac{d g}{d x}
\end{align}
We define $u^{(i)}$ as
\begin{align}
  u^{(i)} = f^{(i)}(\mathbb{A}^{(i)})
\end{align}
Where $\mathbb{A}^{(i)}$ is the set of all nodes that are parents to $u^{(i)}$. This is the notation from \cite{Goodfellow-et-al-2016}, which we are basing this section on.

Since we are looking at graphs, only parents of the node serve as input to the function associated with that node.
We consider a graph describing how to compute a scalar $u^{(n)}$ at node n, this scalar is the one we want to find the gradient of with respect to its $n_i+n$ ancestors $u^{(1)}$ to $u^{(n_i)}$ in the graph, ie. we want to find $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ for $i \in \{1,2,\dots, n_i\}$. 
With the assumption that the nodes in the graph has been ordered in a way that we can compute the output $u^{(t)}$ for $t \in \{n_i+1, n_i+2,\dots, n\}$ one after the other.

We can per the chain rule, compute partial derivitives as
\begin{align}
\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i : j \in \text{Pa}(u^{(i)})} \frac{\partial u^{(n)}}{\partial u^{(i)}} \frac{\partial u^{(i)}}{\partial u^{(j)}}
\end{align}

Using the above we can derive the pseudo code for the back propagation algorithm
\begin{algorithm}[H]
  \caption{Simplified Backpropagation Algorithm }\label{alg:simple-backprop}
  \begin{algorithmic}[1]
  \State use   $u^{(i)} = f^{(i)}(\mathbb{A}^{(i)})$ to obtain the activations of the network.
  \State Initialize \texttt{grad\_table}, a data structure that will store the derivatives that have been computed.
  \Statex \hspace{1.5em} The entry \texttt{grad\_table}[$u^{(i)}$] stores \( \frac{\partial u^{(n)}}{\partial u^{(i)}} \)
  \State \texttt{grad\_table}[$u^{(n)}$] $\gets 1$
  \For{$j = n-1$ \textbf{downto} $1$}
    \Statex \hspace{1.5em} Compute:
    \[
      \frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i : j \in \mathrm{Pa}(u^{(i)})} \frac{\partial u^{(n)}}{\partial u^{(i)}} \cdot \frac{\partial u^{(i)}}{\partial u^{(j)}}
    \]
    \State \texttt{grad\_table}[$u^{(j)}$] $\gets \sum_{i : j \in \mathrm{Pa}(u^{(i)})}$ \texttt{grad\_table}[$u^{(i)}$] $\cdot \frac{\partial u^{(i)}}{\partial u^{(j)}}$
  \EndFor
  \State \Return $\{\texttt{grad\_table}[u^{(i)}] \mid i = 1, \dots, n_i\}$
  \end{algorithmic}
\end{algorithm}


\subsection{Gradient descent}
Gradient descent is an optimization algorithm, it converges towards a minimum by moving in the direction of the negative gradient. This section is follows \cite{ruder2016overview} explaination of gradient descent and it's variants.
There is the three primary types of gradient descent:
\begin{enumerate}
  \item \textbf{Batch gradient descent:}
  This is the standard gradient descent method. It computes the gradient of the loss function w.r.t. the parameters of the loss function for the entire dataset.
  Since it has to calculate gradients on the entire dataset it can be very slow and in a lot of cases the dataset will be to big to fit into memory, which will cause the program to crash.
  \begin{align}
  \theta = \theta - \eta \cdot \nabla_\theta L(\theta)
  \end{align}
$\theta$ represents the parameters / weights, $\eta$ is the learning rate and $\nabla_\theta L(\theta)$ is the gradient of the loss function w.r.t. the parameters.
This method will always converge to a local minimum. 

  \item \textbf{Stochastic gradient descent (SGD):}
This is a common used method for updating parameters, it updates the parameters for each sample in the dataset
\begin{align}
\theta = \theta - \eta \cdot \nabla_\theta L(\theta; x^{(i)}; y^{(i)})
\end{align}
This saves a lot of computations and is therefore faster, because it doesnt have to do them on the entire dataset. It does however have the effect that the more frequent updates has a higher variance, which causes the weights to fluctate.
These fluctuations can cause the parameters to jump to a different local minimum and escape the crevasse it was in. The high variance can make it harder for the parameters to converge to a minimum, but this can be combated by decreasing the learning rate over time. It has to be mentioned that for each epoch we shuffle the dataset, as to avoid the model learning the order of the dataset and to keep it unbiased.

  \item \textbf{Mini-batch gradient descent:}
  Minibatch gradient descent is a combination of both of the above. It takes more than just a single sample and less than the entire dataset. This deals with the problems in both methods, it reduces the computation time, helps with memory management and reduces variance, which helps with convergence.
  \begin{align}
    \theta = \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i:i+n)}; y^{(i:i+n)})
  \end{align}
The batch size can vary greatly, the smaller the size the less computations and memory use, but also results in higher variance and vice versa.
When using mini-batches different deep learning libraries leverages super optimized linear algebra methods combined with parallel computations of samples using GPU's.
\end{enumerate}
\subsubsection{ADAM}
The Adaptive Moment Estimation (ADAM) method is used for computing adaptive learning rates of the individual parameters.
\begin{align}
  m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
  v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
\end{align}
$m_t$ and $v_t$ are estimates of the mean and the variance of the gradients respectively. As we can see per the formula, the estimate of the moments are dependent upon the estimate of moments of previous timesteps. $m_t$ and $v_t$ are initialized as vectors of zero thus making them biased towards zero, the bias is extra present during initial time steps and when $\beta_1$ and $\beta_2$ are close to one.
ADAM combats these biases by using bias-corrected mean and variance estimates.
\begin{align}
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t}
\end{align}
This gives the following ADAM update rule:
\begin{align}
  \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{align}
\section{Methodology}
This study uses a ConvLSTM model to predict the future growth of mycelium. The methods include extensive data preprocessing, choice of model architecture and hyperparameters, training with a combined loss function with varying weights aswell as an evaluation based on visual inspection and error functions for prediction precision.
\subsection{Data}

\subsubsection{Data collection}
The dataset \cite{andyg} was found on the open source platform Figshare, where it was published by Andy Goldsmith. The dataset was collected over six different trials, for each trial they grew three batches each captured with its own camera. The image collection was done by having the yeast grow on agar plates with growth medium Yeast Extract-Peptone-Glycerol. The agar plates was organized into grids, to save space and streamline the documentation proces, they were placed inside an incubator as to prevent contamination of the mycelium. The images was captured with a camera placed on a motorized cart on a track inside of the incubator. Photos was taken in a frequency rate that is meant to capture the critical parts of mycelium growth behavior, which means time between each frame is in the span of hours, each colony grew over several days, most frequently around three days.
\subsubsection{Data organization}
The dataset consists of images of the yeast \textit{Saccharomyces cerevisiae}, which is the common yeast used in brewing. It is time series data so each image represents a frame in a timelapse of a yeast colony growing. There is 5500 colonies, 196 unique strains of the yeast species and in total there is 933103 images existing in the dataset. The image file names are named in a manner so it's easy to identify which strain and experiment iteration it belongs to, further the growth time for the yeast at time of capture is recorded. File names are of the form: "YPG11407\_001\_1080.jpg", YPG is the growth medium, 11407 is the strain, 001 is the experiment iteration and 1080 is for how long it has been growing in minutes.

\subsubsection{Data preprocessing}
There were a couple problems identified with how the data were organized, when loading in data, there is no meta data descriping the order of the images in the directory (which was made so frames was in chronological order etc.), when loading in the files the Pytorch program therefore went to the default and sorted files alphabetically, which is a problem because it would then sort images at time 1200 before images at time 200. 
So i wrote a program that stores the strain, experiment iteration and time frame and then sorted numerically from lowest to highest instead of alphabetically. Another problem i ran into was that because im using a convolutional neural network, all the image resolutions has to be the same which they were not due to the fact each image are cut to the edge of the hyphae expansion and since the hypaes expand for each image they all had different resolutions.
 I found the highest width and height in the data and then padded all the images with black pixels so they all had the resolution Max Width x Max Height, i padded them in a way that kept the original image centered. I then grouped the images into sets of strain and experiment iteration so that the frames is sorted as a sequence. 
 I then split each sequence into input frames and target frames. The same number of frames that needs to be predicted, the same number of target frames is needed for training. Say we want to predict $80$ frames ahead then $80$ target frames will be stored and removed from the sequence, the remaining sequence is then the input sequence.

 To be able to train the model i had to make sure that each batch of sequences had the same number of frames. This is already fullfilled for the target sequence, but the input sequence will have varying lengths. therefore i also padded the input sequences, by finding the max number of frames for the input sequences in a batch and then added black images at the end of the rest of the input sequences in that batch, so the lengths match.

The entire dataset is split up into three segments, a training set, a validation set and a test set, each respectively consisting of $70\%$, $15\%$ and $15\%$. This part is crucial when it comes to the methodology of Machine learning. Since we are using data to train the model, that data cannot be used to evaluate the model because the model is fitted to that exact data, so the error will always be low, granted you made a good model. The only real way to test if a model is good is to validate it on some data that it has not been trained on. Therefore we keep a segment of the data as validation data and keep checking our produced models on that validation data, to make sure we are not overfitting and it captures the underlying model that distributes this data. One has to be careful because once some data has been used for validation and we change parameters according to that validation process, the model now becomes biased towards that data and the model and the validation data is no longer independent. Thats why we keep a portion for testing, this is the part of the dataset that has to be locked away, while models are being created, because once we use it for testing, we can no longer make changes to the model and make an unbiased evaluation of the model using that data. We are essentially burning data everytime we use it to test the models ability to capture the underlying structure of the data. The more we change our models based on the errors we see in the validation set the more biased that data and our models become, which is not good, because ideally we want the model to be as unbiased as possible so we know how it will perform on new data.
\subsection{Reasoning behind choices}
models, algorithms, optimizers, loss functions, evaluering etc.
\subsubsection{ConvLSTM}
In this study we use the ConvLSTM model. The reasoning behind this is simple:
\begin{enumerate}
  \item The model is capable of learning spatiotemporal patterns. It can do so locally which is great for mycelium data since it only spreads to neighbouring pixels. 
  \item It is combining the strengths of two basic strong deep learning methods. Due to the capabilities of the LSTM it processes sequences chronologically by having hidden states over time, which is advantageous because growth happens step by step. CNN is considered one of the strongest methods for image analysis. So the combination of these models is rather sensical.
  \item ConvLSTM is super memory efficient compared to other spatiotemporal models. Because LSTM conserves weights over time because its reccurent. Thanks to the convolutional operator, it is possible to retain the native data structure and use few weights for the kernel. ConvLSTM can do step wise prediction, meaning it can generate one frame at a time, each based on the previous hidden state. The each predicted frame is then collated into a sequence, this is in contrast to predicting the entire sequence at once. This makes it so longer sequences can be predicted without worrying about running out of memory.
\end{enumerate}
\subsubsection{Backpropagation}
The choice of backpropagation was straight forward.
\begin{enumerate}
  \item It is efficient in calculating the gradients w.r.t. the parameters.
  \item It works well with gradient descent.
  \item Scales well and works for all computation graphs.
  \item It is the most widely implemented algorithm in deep learning libraries and is really the standard.
\end{enumerate}
\subsubsection{Gradient descent: ADAM}
Considerations with gradient descent
\begin{enumerate}
  \item It is the most implemented method
  \item Simple and effective, scales well
\end{enumerate}
Reasoning behind ADAM
\begin{enumerate}
  \item Has momentum which is great, because it remembers earlier gradients, which smooths out updates.
  \item Works well for sequences and for mini-batches.
\end{enumerate}
\subsubsection{Loss functions: Dice and BCE}
BCE with a sigmoid layer is a relatively normal loss function to use for binary segmentation, which is much a like to what we are doing.
\begin{enumerate}
  \item It's great at measuring each pixel and for comparing the probability prediction to the target. It does so in a way that strongly directs the loss by punishing wrong predictions hard.
  \item Works directly at what we are doing, which is predicting a binary probability for each pixel in the image.
\end{enumerate}
The Dice loss is a mathematical translation of what we are trying to do, which is to make the mycelium predictions overlap the target.
\begin{enumerate}
  \item Dice loss is good for data with disproportionate background and foreground ratio. It only focuses on the overlap between the foreground of the target and the prediction.
\end{enumerate}
By combining these losses we get the best of both worlds. The BCE which is focusing hard on correct predictions and the dice loss which is more forgiving and works well for disproportionate datasets. Both losses use sigmoid activation so the outputs can be intepreted as probabilities.
\subsection{Software and tools}
To implement the model and load, sort, transform and plot data the programming language Python v. 3.12.2 installed through conda-forge in a conda enviroment.
For implementation im using the following libraries
\begin{itemize}
  \item \textbf{Standard python libraries:} Operating system interface (OS), Random, multiprocessing.dummy
  \item \textbf{Data handling:} Numerical python (numpy), Python data analysis (Pandas)
  \item \textbf{Plotting and visualization:} Matplotlib.pyplot
  \item \textbf{Image and videos:} OpenCV (cv2), Pillow (PIL, Image, ImageOps)
  \item \textbf{Progress bar:} Taqaddum (tqdm)
  \item \textbf{Pytorch:} torch, torch.nn, torch.nn.functional, torch.optim, torch.utils.data, torch.cuda.amp
  \item \textbf{TorchVision:} torchvision.transforms, torchvision.datasets
  \item \textbf{Scientific computing:} Scientific Python (SciPy, scipy.ndimage.distance\_transform\_edt)
  \item \textbf{Image morphology:} Scikit-image morphology module (skimage.morphology)
\end{itemize}
\subsection{Hardware}
Data manipulation, model creation and training happens in python and jupyter notebook files. For testing and visualization jupyter notebook is used, for training we run the python file from terminal on a cloud computer.
The training happens on a cloud computer running an AMD EPYC 9454 CPU with 48 physical cores / 192 threads and 2.75 GHz combined with a NVIDIA H100 GPU and 756 GiB RAM. This is an incredibly strong computer, the GPU is the state of the art when it comes to training AI models and it has the ram and CPU to support it. The reason this is so important is because AI models can be trained using parallel operations on GPU's which makes the training process 10x-100x faster.
With a dataset of this magnitude and a rather memory hungry model, training on a regular computer is not considered viable. The test runs and data manipulation aswell as visualization is done on a regular computer however.
\subsection{Training loop}
\section{Experiments and analysis}
\subsection{Model}
This is the Architecture of each cell in the implemented ConvLSTM model:
\begin{align}
  i_t &= \sigma(W_{i} * [\mathcal{X}_t, \mathcal{H}_{t-1}]) \\
  f_t &= \sigma(W_{f} * [\mathcal{X}_t, \mathcal{H}_{t-1}]) \\
  \hat{\mathcal{C}_t} &= \tanh(W_{c} * [\mathcal{X}_t, \mathcal{H}_{t-1}])\\
  \mathcal{C}_t &= f_t \odot \mathcal{C}_{t-1} + i_t \odot \hat{\mathcal{C}_t} \\
  o_t &= \sigma(W_{o} * [\mathcal{X}_t, \mathcal{H}_{t-1}]) \\
  \mathcal{H}_t &= o_t \odot \tanh(\mathcal{C}_t)
  \end{align}
  As we can see it has no bias weights and no peephole connections. 
  Theres is also a difference in the input, we concatenate $\mathcal{X}_t$ and $\mathcal{H}_{t-1}$ a long the channel dimension and get this: $[\mathcal{X}_t, \mathcal{H}_{t-1}]$. $\mathcal{X}_t$ and $\mathcal{H}_{t-1}$ has shape $(batches, C_{hidden channels}, H, W)$ so when we concatenate $[\mathcal{X}_t, \mathcal{H}_{t-1}]$. $\mathcal{X}_t$ has shape $(batches, C_{hidden channels}+C_{hidden channels}, H, W)$.
  This is more efficient because we only need one convolution per gate, it simplifies the code and the end result is the same. 

  The cell architecture as implemented in python
  \begin{lstlisting}
    class ConvLSTMCell(nn.Module):
    def __init__(self, hidden_channels, kernel_size):
        super(ConvLSTMCell, self).__init__()
        self.hidden_channels = hidden_channels
        self.kernel_size = kernel_size

        #Convolution LSTM gates (input already projected to hidden_channels)
        self.conv_i = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)
        self.conv_f = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)
        self.conv_o = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)
        self.conv_c = nn.Conv2d(hidden_channels + hidden_channels, hidden_channels, kernel_size, padding=1)

    def forward(self, x, h_prev, c_prev):
        combined = torch.cat([x, h_prev], dim=1)  #Concatenate
        i_t = torch.sigmoid(self.conv_i(combined))
        f_t = torch.sigmoid(self.conv_f(combined))
        o_t = torch.sigmoid(self.conv_o(combined))
        c_t_hat = torch.tanh(self.conv_c(combined))
        c_t = f_t * c_prev + i_t * c_t_hat
        h_t = o_t * torch.tanh(c_t)
        return h_t, c_t
  \end{lstlisting}
  Each gate is a convolutional layer followed by an activation function. This cell is called in the implementation of the full ConvLSTM model.

The full model combines the cells in a meaningful manner, which in this case is a chronological one, so it follows the theory behind the ConvLSTM but also so it is capable of solving the problem of this thesis. The solution being a greyscale image prediction sequence of the mycelium with constant length. 
The model takes a number of arguments, these are parameters that decide depth, width etc. of the model. The forward, where each component of the model is called, the model is assembled. The pipeline is as follows:
The model is fed the input sequence, each frame is processed chronologically. First the channel of the frame is send to the input\_adapter, where it is broadcasted to $16$ hidden channels, so the cell can process it. These $16$ hidden channels are then used as input to the cell. The cell, based on that input and the cell state, computes another $16$ hidden channels and an updated cell state. For each $layer > 1$, the computed hidden channels and cell state is then used again for input to the cell. This is done for each frame in the input sequence.
When all the frames in the input sequence has been processed, the last hidden channels and the last cell state is used to make predicitons. When making predictions it is exactly the same thing happening, except the cell is not feed the input sequence, but instead just the last hidden channels for each layer and frame. Then for each frame the output\_adapter is called, it takes the hidden channels as input and reduces them to one channel representing the greyscale channel, this is the result of the output\_adapter: $(B, 16, H, W) \rightarrow (B, 1, H, W)$.
  \begin{lstlisting}
    class ConvLSTM(nn.Module):
    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers):
        super(ConvLSTM, self).__init__()
        self.hidden_channels = hidden_channels
        self.num_layers = num_layers

        #Project input_channels -> hidden_channels
        self.input_adapter = nn.Conv2d(input_channels, hidden_channels, kernel_size=1)

        #Project hidden_channels -> output channel (1 for grayscale prediction)
        self.output_adapter = nn.Conv2d(hidden_channels, 1, kernel_size=1)

        #Stack ConvLSTM cells (same input/output channels)
        self.cells = nn.ModuleList([
            ConvLSTMCell(hidden_channels, kernel_size)
            for _ in range(num_layers)
        ])

    def init_hidden(self, batch_size, spatial_size):
        H, W = spatial_size
        device = next(self.parameters()).device
        h = [torch.zeros(batch_size, self.hidden_channels, H, W, device=device) for _ in range(self.num_layers)]
        c = [torch.zeros(batch_size, self.hidden_channels, H, W, device=device) for _ in range(self.num_layers)]
        return h, c

    def forward(self, input_sequence: torch.Tensor, future: int = 0) -> torch.Tensor:
        B, T_in, _, H, W = input_sequence.size()
        h, c = self.init_hidden(B, (H, W))
        
        #Process input sequence to update hidden state
        for t in range(T_in):
            input_t = self.input_adapter(input_sequence[:, t])
            for i in range(self.num_layers):
                cell_input = input_t if i == 0 else h[i - 1]
                h[i], c[i] = self.cells[i](cell_input, h[i].detach(), c[i].detach())

    
        outputs = []
        input_t = h[-1]
    
        for t in range(future):
            for i in range(self.num_layers):
                cell_input = input_t if i == 0 else h[i - 1]
                h[i], c[i] = self.cells[i](cell_input, h[i], c[i])
            input_t = h[-1]
            output_frame = self.output_adapter(input_t)
            outputs.append(output_frame)
    
        return torch.stack(outputs, dim=1)  #[B, T_future, 1, H, W]
  \end{lstlisting}
  The number of predictions depend on the value of the integer $T\_future$. The outputs $[B, T_future, 1, H, W]$ will typically not have values in $[0,1]$ as for greyscale, this is because of the sigmoid layer in the loss function. 
\subsubsection{Model architecture}
The model can be constructed in many different sensical ways. The models for this project will have $1$ to $3$ layers. Each layer will consist of a ConvLSTM cell with the convolution layer gates. The model utilizes two types of $1 \times $ convolutional projection layers, the output\_adapter and the input\_adapter, made for projecting back and forth between hidden channels and input channels. This is what the core structure of the model looks like.

For the thesis there has been constructed three models, each with respectively $16$, $24$ and $32$ hidden channels. The kernel size is constant for all models and is set at $3 \times 3$. All greyscale image tensors has to have the same height and weight dimensions in order for the model to process them. The max height is $371$ and the max width is $408$, images of this size is considered big. In order to combat the size issue, there is added a scale factor parameter that scales all the images down. This dramatically reduces the amount of padding that has to be done and the memory it takes to load the data.
The input channel for all the data is greyscale, which sets $C_{in} = 1$. The length of the input sequence is calculated based on the length of the output sequence, $T_{in} = length(sequence) - T\_future$. The number of batches loaded in at once depends on the size of the model, if the model is wide or deep, the batch size has to be smaller, so the memory usage equals out. Usually the batch size is $B=10$ and for deep or wide models $B=5$. The input dimensions are then $[B = 10 \text{  or  } 5, T_{in}, C_{in}=1, H=371\lambda_{scale}, W=408\lambda_{scale}]$.
The number of predicted frames / length of the output is a constant variable $T\_future$. Experiments include models with $T\_future = 20$, $T\_future = 40$ and $T\_future = 80$. The models architecture is autoregressive, which means each predicted frame is fed into the next time step. This is opposed to multiframe prediction, where multiple frames are used. In addition the model is also unidirectional, meaning there is no backward pass over time, it only goes forward. The activation functions are consistent with the traditional ConvLSTM structure.

\subsection{Data optimization}
Data optimization is crucial for good results in machine learning. The models are trained on the data and learns the patterns that it can find in the data, so it minimizes the loss. The problem for this thesis is to predict the growth of a mycelium network, therefore the data should reflect that network growth in a way that is sensical for the prediction. It is evident from the raw images, that there is some noise and the small nuances of white makes it difficult to distinguish between the mycelium hyphae / root network and the non-hyphae white background region, that the main hyphaes grow on.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/raw_target.png}
  \caption{Visualization of the last $10$ frames in a non manipulated image sequence. $\lambda_{scale}=0.5$, dilation with disk(1)} % Dummy caption generated using lipsum
  \label{fig:n11}
\end{figure}

A decision has been made to binarize the data by using adaptive thresholding and then skeletonize using the algorithm in T. Y. Zhang and C. Y. Suen's paper \cite{10.1145/357994.358023}. To use this algorithm succesfully the data must be altered. Firstly data is converted from $[C_{in}, H, W]$ to $[H, W]$. Image is then cleaned by setting all $values \leq 2$ to be equal to $0$. The cleaned image is then processed by gaussian blur, this applies a gaussian kernel across the image which blurs the image out more, making it less variant. Each entrance in the cleaned image is then divided by each entrance in the gaussian blur image $R(i, j) = \frac{I(i, j)}{\text{GaussianBlur}(I)(i, j) + \epsilon}$.
The gaussian blur is capturing the illumination difference on the images and by dividing by it attempts to smooth out this difference. Values are then normalized to fit greyscale in the range $[0, 1]$. Now to highlight the hyphae Contrast Limited Adaptive Histogram Equalization (CLAHE) is applied to the image. CLAHE in this thesis is used as a black box. Briefly explained it amplifies contrasts locally, which is good for faint hyphae and hyphae in varying lighting. The altered image is then binarized using adaptive thresholding, it has to be adaptive because of the variable lighting and possible difference in concentration of hyphae. It works like this: $\text{binary}(i,j) = 
\begin{cases}
1 & \text{if } I(i,j) > \text{mean}(N_{i,j}) \\
0 & \text{otherwise}
\end{cases}$, it finds the mean of a local region and then zeroes out all entrances with $values \leq \text{mean}(N_{i,j})$. To fix holes and gaps in the binary image we use the \href{https://github.com/scikit-image/scikit-image/blob/v0.25.2/skimage/morphology/gray.py#L443-L517}{Closing} function. The function is treated as a black box, it serves the purpose of making the hyphae continous and the mycelium network connected. The next step is to skeletonize the binary image so each hyphae is of the same thickness. At last the image is transformed by \href{https://github.com/scikit-image/scikit-image/blob/v0.25.2/skimage/morphology/gray.py#L245-L363}{Dilation}, this makes the skeleton thicker, the goal of this is to balance the proportions of foreground and background pixels.
 The image is then resized according to the scale factor $\lambda_{scale}$ and then padded to be of size $(height_{max}\lambda_{scale} \times width_{max}\lambda_{scale})$. 
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/skeleton_target.png}
  \caption{Visualization of the last $10$ frames in an optimized image sequence. $\lambda_{scale}=0.5$, dilation with disk(1)} % Dummy caption generated using lipsum
  \label{fig:n12}
\end{figure}

To tackle the problem of sparse foreground pixels further measures has to be taken. One of the factors for the low proportion of foreground pixels stems from the data optimization pipeline, where we remove a big chunk of what was considered foreground initially. However the main factor is the max size padding, this adds all around a lot of extra background. The amount of padding is of course dependent on each sample, but based on visual inspections, it seems like a good chunk of the data is quite a bit smaller than the max size. The skeleton has already been dilated, which is a good first step. Secondly we want to find a mask that masks the relevant part of the image. This mask can then be passed to the loss functions, that then can be computed on only the masked area.
The mask is found by setting a threshold, if $value>0$ then $value = 1$ else $value = 0$. Since all $padding = 0$ it will not be a part of the mask, one could argue that there could be some relevant background which has $value = 0$, but this is assumed to be very unlikely since the raw data images has not been cleaned.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/mask_target.png}
  \caption{Visualization of the last $10$ masks for the frames in an image sequence.} % Dummy caption generated using lipsum
  \label{fig:n13}
\end{figure}

After all the data optimization the foreground percentage with and without the mask, across the entire target training set are as follows:
\begin{align*}
  \textbf{For $\lambda_{scale} = 0.5$ and no dilation}\\
  \frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 24.12\%\\ 
  \frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 5.61\%\\
  \textbf{For $\lambda_{scale} = 0.5$ and dilation with disk$(1)$}\\
  \frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 36.31\%\\ 
  \frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 8.44\%
\end{align*}
This shows a much healthier percentage of foreground, which implies a more balanced dataset. The settings $\lambda_{scale} = 0.5$ and no dialation, is the closest to $25\%$ foreground and $75\%$ background, which is the desired ratio. Just because dilation is not needed when $\lambda_{scale}=0.5$, doesnt mean it is not useful, with $\lambda_{scale}=1$ dilation is much needed to balance the dataset out.

At first i processed the images with dilation after the resizing, this made the data especially vulernable to changes in the scale factor. This meant that the foreground would become to dominant, this was the resulting percentages:
\begin{align*}
  \textbf{For $\lambda_{scale} = 0.5$ and post resizing dilation, with disk$(1)$}\\
  \frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 45.63\%\\ 
  \frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 10.61\%\\
  \textbf{For $\lambda_{scale} = 0.5$ and post resizing dilation with disk$(2)$}\\
  \frac{\text{total amount of foreground}}{\text{total amount of masked pixels}} = 60.12\%\\ 
  \frac{\text{total amount of foreground}}{\text{total amount of pixels}} = 14\%
\end{align*}
Since the forecast horizon can be rather long $T\_future = 20, 40, 80$, it is necessary to make sure the remaining frames for the input is long enough to be meaningful. The minimum length for an input sequences is sat at $20$ frames, this is still rather short, but depending on the sequence, it could be long enough to be meaningful.
\subsection{Prediction}
\subsubsection{Foreground-to-background proportion}
To test the effect of pixel foreground and background ratios effect on ConvLSTM, we run three models respectively with $1$, $2$ and $3$ layer(s). They are ran on the three datasets proposed in the Data optimization section, it is the datasets with $24.12\%$, $36.31\%$ and $45.63\%$ foreground percentage, they will be refered to as dataset 1, dataset 2 and dataset 3.
The first model that is ran is the ConvLSTM as decribed in Model section, it is ran with $1$ layer and $32$ hidden channels. The figure below showcases the first frame in its prediction sequences next to the ground truth target image.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/bfcomp1.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n14}
\end{figure}
The red square is the mask that is applied to the loss function, this means that only the prediction contained within that red square is considered, both in regards to evaluation and training. 
Viusally, when looking at figure 11, the model seems to capture the characteristics of the mycelium pattern, it does so with varying conviction. On dataset 1, the gaps between the hyphae is not as distinct as with the other two, this is interpreted as the prediction being less sure of where the hyphae actually is. I think the prediction on dataset 1 and 2 is comparable to each other, both captures features rather well and seems to do so with the same conviction. Looking at the validation data in the table, it is evident that dataset 2 has the lowest validation loss and the highest dice score.
\begin{table}[H]
  \centering
  \caption{Model Comparison: (1 Layer)}
  \begin{tabular}{|c|c|c|c|}
  \hline
  Metric & 45.63\% & 36.31\% & 24.12\% \\
  \hline
  Train Loss & 0.1813 & 0.2138 & 0.2608 \\
  Val Loss   & 0.1899 & 0.2221 & 0.2649 \\
  Dice Loss  & 0.1699 & 0.1979 & 0.2490 \\
  \hline
  \end{tabular}
\end{table}
The next model has $2$ layers and $32$ hidden channels, it is a slight deeper model than the one before. Right out of the gate looking at figure 12, it is seen that the model is likely underfitting to the data. This is typical for a high foreground percentage, because its more likely to be correct if guessing over a broad amount of pixels than if the image had a low foreground percentage. On the other hand the model seems to be fitting quite well to dataset 1 and 2, when comparing ground truth to predictions. The background is a dark grey, meaning it isnt averaging its guesses, but tries to capture the pattern of the mycelium. It seems like the contrast between background and foreground is the highest on dataset 1, which indicates a more confident model. 
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/bfcomp2.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n15}
\end{figure}
As presented in the table, the validation loss for the model is lower on dataset 2 than the others. One of the reasons why it might be lower than dataset 1's model, is that there is more foreground to be correct on. An explaination is that by guessing uniformly there is a higher chance to be correct and since wrong guesses are punished harder due to BCE, it has a lower loss.

\begin{table}[H]
  \centering
  \caption{Model Comparison: (2 Layers)}
  \begin{tabular}{|c|c|c|c|}
  \hline
  Metric & 45.63\% & 36.31\% & 24.12\% \\
  \hline
  Train Loss & 0.2119 & 0.2095 & 0.2594 \\
  Val Loss   & 0.2127 & 0.2170 & 0.2635 \\
  Dice Loss  & 0.1844 & 0.1875 & 0.2433 \\
  \hline
  \end{tabular}
\end{table}
The third model with $3$ layers and $32$ hidden channels has more varying results. Once again it is underfitting on data 3 and therefore is capturing none of the complexity. The model trained on dataset 2 seems to be struggling with confidently predicting the hyphae. This is likely due small gaps in between the hyphae and the high foreground density. Visually the model trained on dataset 1 looks superior, it mimicks the ground truth mycelium pattern and might be "anticipating" the growth of the mycelium in future frames by having a faded border around the mycelium. 
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/bfcomp3.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n16}
\end{figure}
However once again, according to the table, the validation loss is lower and the dice score is higher for the model trained on dataset 2.
\begin{table}[H]
  \centering
  \caption{Model Comparison: (3 Layers)}
  \begin{tabular}{|c|c|c|c|}
  \hline
  Metric & 45.63\% & 36.31\% & 24.12\% \\
  \hline
  Train Loss & 0.2127 & 0.2061 & 0.2542 \\
  Val Loss   & 0.2133 & 0.2172 & 0.2623 \\
  Dice Loss  & 0.1844 & 0.1827 & 0.2348 \\
  \hline
  \end{tabular}
\end{table}
ALl future models are all trained on dataset 3, meaning the dataset with no dilation and a foreground percentage of $24.12\%$.
\subsubsection{Depth}
In this section we will be examining three models each with varying depths, the three models has $1$, $2$ and $3$ layers respectively. The models are identical in all other regards, they have 32 hidden channels, same loss function and prediction length. Since the model with three layers uses more memory we had to decrease the batch size to $3$ for the training of that specific model. These number of layers have been choosen because biological skeletons usually dont require deep models to learn and there is also a memory restriction on the hardware the models are trained on. We will be examining the initial frame prediction and a thresholded binarized version of the prediction compared to the target. Further we will be discussing, how the prediction sequence changes as it progresses in time, all sequences can be found at \cite{gagarahn2025bachelor} as GIF-files.

It looks like the first $1$ layer model made a prediction that is a decent fit to the target as you can tell by the figure. You can tell the prediction is a bit fuzzy and observing the thresholded prediction, it's evident that the predicted hyphae is quite a bit thicker than that of the target. The entire predicted sequence is available as a GIF-file in the GitHub repository associated with this thesis \cite{gagarahn2025bachelor}.
I would say the predicted sequence even performs decently in the later frames, it is not identical to the target, but it is predicting that the mycelium keeps expanding. It seems to be unable to single out any new pattern of the growing hyphae, but instead just makes a solid rather uniform prediction around the established pattern.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L1_H32_T80.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n17}
\end{figure}
The prediction in figure 18, that was produced by the two layer model, initially seems visually worse than the prediction by the $1$ layer model, due to how the thresholded version is looking. However it seems to be catching some details the other model didnt. It has learned that the mycelium grows from the center which creates a circular growth pattern, we can tell this by the grey cloud around the complex pattern. This is however the only trait it is keeping as the sequence goes on, because it is forgetting the intricate net-like pattern of the hyphae. Instead it fades into a grey cloud and only by observing the treshold can you see it is keeping its circular prediction.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L2_H32_T80v1.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n18}
\end{figure}
The thresholded image of the prediction shown in figure 19, made by the $3$ layer model, looks close to that of the target, although it has a bit thicker hyphae. When just looking at the raw prediction a few details stand out. The model has predicted that the future frames are gonna be circular and close around the initial mycelium net. Also the background is darker than that of the prediction in figure 14, this means it is more confident that there wont be any foreground there. Looking at the GIF for the entire sequence, we observe that it also struggles from blurring into a grey cloud, however the net-like pattern is still visible, just more faint. Examining the thresholded version, one can see that it is not just blurring completely out, it still maintains a decent prediction for what should be considered foreground. Not optimal for pattern recognition but it does predict accurately that it grows and the way it grows, although in the end it overpredicts how much growth will happen.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L3_H32_T80.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n19}
\end{figure}
The two models that stand out, when doing the visual evaluation, is the ones with $1$ layer and $3$ layers. 
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/comparison_loss_plot_depth.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n20}
\end{figure}
Looking at figure $17$, it is observed that the model with $3$ layers reaches the lowest minimum validation loss and also the lowest test and dice loss compared to the two other models. Based on the visual evaluation and the loss function statistics, the $3$ layer model is the one we will use for further research.
\subsubsection{Width}
In this section, we will be examining three predictions made by three models with varying width in the hidden channels, further we will be looking at the loss statistics for the three models. It is models with $3$ layers, $8$, $16$ and $64$ hidden layers and predicting sequences of length $80$, all trained and evaluated on the non dilation dataset.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L3_H8_T80.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n21}
\end{figure}
The $8$ hidden channel models prediction is not capturing any patterns or complexities, it is making a uniform guess, this suggests underfitting. The claim is further supported by the loss statistics in figure $21$. $8$ hidden channels is considered shallow for a neural network and it might not be enough to capture the complexities of mycelium networks.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L3_H16_T80.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n22}
\end{figure}
The prediction made by the $16$ hidden channels model is blurry initially, but manages to keep its form and rather accurately depict the growth of the target. This is especially true for the threshold version of the prediction. It keeps the seperations until a pretty late stage and the outline is not far off from the target. Overall the prediction suggests that the model is not as good at making as accurate predictions as the others early on, but is more stable over time.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L3_H64_T80.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n23}
\end{figure}
Examining figure $20$, showcasing the prediction made by the $64$ hidden channels model, it can be seen that the model is confident in its prediction. There is high contrast between the hyphae and the background, you can even see, what i assume to be more faint hyphae, being less white because it only will be included in the target some of the time, due to the optimization pipeline. The mycelium net is not fuzzy and the treshold is seemingly accurate to the target, except for some extended hyphae which show up on later in the target sequence.
The model seems to preserve its complexity better than any previous model, the threshold still has the distintion between foreground and background in the end. It is still struggling with blurring out in later frames and also over extending its spread / growth.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/comparison_loss_plot_width.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n24}
\end{figure}
According to the loss statistics in figure $21$ the model with $16$ hidden channels performed comparable to the one with $32$ hidden channels and visually they also express the same behaviour. The $64$ hidden channels model out performs all previous models, when it comes to loss statistics. Visually it is also doing the best or the same as the best models, we have looked at so far.
\subsubsection{Forecast horizon}
In this section, we will be evaluating the how effective the model with $3 $ layers and $64$ hidden channels is on different prediction sequence lengths. The models are trained to predict $20$, $50$ and $100$ frames ahead.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L3_H64_T20.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n25}
\end{figure}
Figure $22$ featuring the $20$ frames long prediction made by the model trained with a forecast horizon of $20$. This is the most accurate prediction our model has produced. The first frame in the sequence is very close to the target frame, when applying the threshold, they look almost exactly the same.
 When inspecting the sequence in the GitHub repository it is seen that the model is predicting the mycelium net to expand, it is unable to predict the distinct hyphae, but just predicts a generel expansion around the circular net. The raw prediction is dilluting details, but the complexities of the mycelium patern is still clearly visible.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L3_H64_T50.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n26}
\end{figure}
The $50$ frames long prediction sequence is not showcasing any learned complexities or patterns as evident from figure 23. It is comparable to the model with $8$ hidden channels and the predictions made on the data with higher foreground percentage. It is a sign of underfitting and this claim of underfitting is also supported by the loss statistic in figure $25$. It is rather an odd occurence, since shorter sequences are expected to outperform longer sequences. I did train the model twice with these settings and on the same data and it got stuck in the same local minimum twice.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/L3_H64_T100.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n27}
\end{figure}
For the prediction sequence with a $100$ frames, a not to unexpected behavior is aparrent from figure 24 and the GIF sequence. The predicted growth of the mycelium is not as complex as the target, it is however pretty close in size and shape. One could intepret this as the model minimizing loss by hedging its bet that the mycelium will expand in a circular pattern and not guessing the pattern of the hyphae.
\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/comparison_loss_plot_time.png}
  \caption{} % Dummy caption generated using lipsum
  \label{fig:n28}
\end{figure}
The $100$ frames model performed better than expected in regards to the loss statistics. Visually it is not useful for predicting patterns, but predicting expected covered area it seemed to do well. Another rather unexpecting finding was that the $50$ frames model under performed to such a degree, performing the worst out of all models. The $20$ frame model performed to no surprise very well particularily on the dice loss. None of the models did however prove that they learned the new expanding hyphae growth patterns, but most of them instead made more uniform predictions around the skeleton.
\section{Discussion}

\subsection{Overfitting and Underfitting}
\subsubsection{Overfitting}
A typical indicator of overfitting is that the test and validation loss is significantly higher than the training loss. During the model hyperparameter experiments we did not observe any alarming difference between the training and validation loss. Throughtout the training there is cases of deviation, but they follow each other closely.
For almost all the models the validation loss minimum was very close to the training loss, the parameters associated with the minimum validation loss is the ones that are extracted and used for modelling. This implies that the parameters used are not ones that are overfitted to the training data.
Small datasets can lead to overfitting as well, it is know that many deep learning models are trained on datasets consisting of millions of samples. This is however not the norm in medical and biological modelling, since data is expensive to produce, here a dataset with $5500$ samples is considered large. Only $70\%$ of the data is reserved for training, further some of those samples are removed because they are to short.
Some would also argue that having three layers and 64 hidden channels is complex of a model for simple skeletonized biological data, which also could lead to overfitting. However there is no sign of overfitting in our evaluations of our models.
\subsubsection{Underfitting}
There was clear signs of underfitting in some of the models we ran, this was in the $8$ hidden channels model and the $50$ future frames model. The learning curve was close to constant through the entire training, the validation and training loss was close to each other and they failed at capturing the underlying structure in the data.
More generally speaking on all the models there was problems with the loss not changing that much from beginning to end. This might have been because of the vanishing gradient problem, in the the theory section on LSTM the vanishing gradient problem was adressed and LSTM generally fixed this problem. This is however not entirely the case, the vanishing gradient problem is still there, just to a lesser extent than the first RNNs.
When conducting the experiments on different foreground percentages, some of the deeper models especially showed signs of underfitting. This was likely due to the fact that BCE punishes confidently wrong guesses harshly, so because of the high amount of foreground it was tough to guess correctly and the loss minimizes by not being confident and uniforming its guess. 
\subsection{Deep vs wide networks}
Depth is refering to the number of layers in the network and width is refering to the number of hidden channels. The benefits of deeper models are that they can model temporal data better and it enables compositional representations, meaning it can dedicate each layer to a feature for example edges.
Deeper networks also takes longer to train, they suffer from vanishing gradients, especially true for RNNs and adding layers increase memory usage signifcantly. We observed in our trials that the deeper model had a lower validation and test loss. This could be a result of the decrease in blur due to the temporal efficiency. They also did take longer to train.
The benefits of making a network wider is that they are easier to train than deeper models and they can learn more complex spatial detail. that said it increases parameter count and to much width can make the model overfit. 
Generally i think the models could have benefitted from being wider, it was a consistent problem that the models couldnt predict the spatial complexities of the expanding mycelium network and hyphae.

\subsection{Interpretation of results}
Overall the analysis suggests that deeper and wider models performs better than shallow and narrow models on the prediction of mycelium growth patterns. It was a repeating problem that the models did not capture the specific pattern of the mycelium network but rather just that there would be growth. Now so far the loss statistics has shown that the wider and the deeper the models are the lower the validation, test and training loss goes and since there is no signs of overfitting. I think it is reasonable to say the ConvLSTM is capable of capturing these features, however it would take powerful hardware to train such a model. 
An observation that was made across the board for the prediction was that over time the prediction frames got more and more blurry, this is known as probalistic blur. It could symbolize that the mycelium does not grow in a predictable way but rather it does randomly, this is none the less not likely. We prepose it's because there is an accomulation of error through time since the model is autoregressive and because the BCE punishes wrong guesses to harshly. The failure to accurately predict the hyphae growth could also be attributed to the rather varied and medium sized dataset. 
Even though all the mycelium is that of the \textit{Saccharomyces cerevisiae}, there is still different strains, that might have different growth patterns.
The experiments also indicate that lower foreground percentages are beneficial for the visual appearance of the predictions, however worse for loss statistics.
The best model achieved for the $80$ frames prediction task on the sparse foreground dataset (dataset 3), is the $3$ layer deep and $64$ hidden channels wide ConvLSTM model. It had an average test loss of $0.2426$, this is considered low loss functions using BCE, which is used with a $0.6$ weight in our comboloss. Since BCE punishes confidently wrong guesses harder, the background will contain some probabilities above $0$ and this will count towards the loss, but can easily be sorted using a threshold. An average dice loss of $0.2195$ is also considered good for segmentation tasks, for binary values like ours it means that after the $0.5>$ treshold $78.05\%$ of the predicted pixels and the target pixels overlap. 
Now if we put this in the context of $80$ predicted frames, the model actually performed very impressively. It's not an easy task to predict the future and to do so spatially is even more challenging. There is error accumulation and just an increased amount of uncertainty the further the prediction horizon is and $80$-frame forecast is considered long-term for ConvLSTM.
This becomes clear, if we look at the performance of the model with the same model architecture but trained to predict $20$ frames ahead instead. The loss statistics are $\text{average test loss} = 0.1913$ and $\text{Average Dice Loss} = 0.1621$, this is not considered outstanding for $20$-frame forecasting, but is still in the good to very good performance range.
\subsection{Strength and limitations}
The use of cross-validation is crucial for evaluating unbiased and useful models. The fact that the dataset is split up into three segments and we therefore achieve unbiased evaluation, makes it so the conclusions and statistics we reach are more likeli to be the true reflection of the models performance.
ConvLSTM is a strong model, it severly lessens the impact of the vanishing gradient problem and it utilizes the powerful convolution operator, which has proven itself as one of the most efficient ways for learning spaital features. The data optimization pipeline has proven itself efficient at binarizing and skeletonizing the images in a way that excludes noise. Further the loss function has been choosen in such a way that it is efficient at learning sparse foreground skeleton networks, where detail is important. 
Overall the loss statistics are indicating that the models are well performing and manages to capture a decent amount of features. It shows that it is possible to get good results predicting lengthy horizons. The qualitative results were also promising and showed some recogniztion of patterns, despite not predicting individual hyphae.
The findings of this thesis suggests that spatiotemporal biological systems might not be as simple as many assume they are. Results and matrics got better the deeper and wider the model went. There might be greater advantages to going even deeper and wider than we went in this thesis, which is an exciting finding.
Being able to accurately model the growth of mycelium over time is extremely valuable. The main components of vitamins, aminoacids, enzymes for textile production, detergent and medicine is based on fermentation technology and yeasts are a crucial part of that. Further in the introduction we discussed the link between optimization and mycelium networks, if one manages to accurately predict mycelium growth, then one might be able to use the same model for optimizing networks.

A good dataset is essential for the success of training machine learning models. The dataset we used in this thesis is rather robust there is however some challenges. Mycelium can easily get contaminated or go into hibernation, this prevents the mycelium from growing and showing any real spatial change over a period of time. Some of the samples in this dataset seems to not be growing or change very slowly over time. This is a problem because it makes it so the model has to learn this dimension of growing and non growing mycelium, which can be tough since they act so differently. This could also be a strenght, because the use case for the model increase if it is succesfully can predict which of the two events will happen.
The optimization pipeline is not perfect, there is an element of randomness to what hyphaes gets shown in the final processed image. It might be because of lighting changes or other stuff that makes it so the pipeline doesnt always show the same skeleton network. However it seems like the model averaged out the hyphae, so that faint hyphae that didnt always show up after the processing would still be predicted, just not with the same amount of certainty.
The predictional blur happening over time is creating noise and removes the contrast and complexity of the network. This is a common problem with ConvLSTMs and is certainly a limitation to what is achievable over long prediction horizons. When training the models the training data is shuffled when it is loaded, this ensures variance in the ordering, so the model doesnt learn the order of the training data instead of the underlying distribution. This however also adds an element of randomness to training models, sometimes results cant be reproduced or some models will get stuck in wrong local minima. This is not ideal because it blurs the line between, what is random and what is deliberate, and you can't just rule a model out the first time it is trained.
The main limitation of Deep Learning models is the computational power it takes to train them. In this project we used a state-of-the-art GPU for training and throughout the project saw limitations in memory and computional power. There was a total of 477 GPU hours used to train the models in this project. The required hardware is expensive and hard to come by, the hardware is high energy consuming and the time it takes to train the models, puts limitations on accesibility and research.
\subsection{Improvements}
Ordinary differential equations, transformers, do some strain identification, predict contamination
We would include an experiment where we testet adding the strain as an input, this could make it so the model could have different parameters for different strains, in case they grow differently. One could also add two output dimensions by using a classification head, one to determine the strain of the input and one to determine if the yeast is contaminated or will see significant growth. 
Further a mask for the temporal padding could improve the learning of long-term dependencies. One could also make the ConvLSTM bidirectional meaning it can consider past and future context. It could also be interesting to compare autoregressive models to non-autoregressive models, the tests suggests that there is quite a bit of error accumulation, maybe using non-autoregressive architecture could prevent this.
Transformers are integral to the deep learning revolution we have seen in Natural Language Processing. They are highly versitile and has shown great promise in learning long-term dependencies because they dont have the vanishing gradient problem. There is a lot of different transformer architecture for learning spatiotemporal dependecies and we think it could be exciting to test these against the ConvLSTM.
\section{Conclusion}
This thesis examined different architectures of the ConvLSTM model for forecasting skeletonized and binarized mycelium growth with a long prediction horizon. The autoregressive models with 1, 2 and 3 layers, $32$ hiddenchannels, $1$ greyscale input channel, $80$-frame forecast, $3 \times 3$ kernel, $1$ stride, $2$ wide zero padding, no bias and no peephole connections, was trained on datasets with varying foreground percentages. The foreground percentage was controlled through dilation and the foreground that examined the best behavior was dataset $3$ with $24.12\%$ foreground, this was concluded mainly on the qualitative results.
Further we compared models with varying widths and depths trained on dataset $3$. Overall most of the models showed good loss statistics compared to the rather long forecast horizon, but the overall theme was that wider and deeper models did better than shallow and narrow models.
 Our best model, with $3$ layers, $64$ hiddenchannels and $80$-frame forecast, saw an average test loss of $0.2426$ and An average dice loss of $0.2195$, this is considered very good for its task. All the produced visual qualitative data produced by the models struggled with, what is likely, accumulation of error as the prediction sequence progresses in time. This is normal behavior for ConvLSTM, none the less it was problematic and made it so the model couldnt accurately predict the complexities of the individual hyphae growth and the expansion of the mycelium net.
 It could be interesting to test deeper and wider model on the data and see if there could be further improvements to the predictions, maybe even add an input dimension for strain type. Transformers have shown incredible performance and versitility, it could be interesting to compare the performance of the ConvLSTM to that of a transformer.
 Fermentation technology is of critical importance in the production of many products, modelling of yeasts growth pattern could improve the fermentation proces further.

\section{Appendix}



\section{Acknowledgements}
Part of the computation done for this project was performed on the UCloud interactive HPC system, which is managed by the eScience Center at the University of Southern Denmark.


\bibliographystyle{plain}
\bibliography{References}  % If you have some references, use BibTeX

\end{document}